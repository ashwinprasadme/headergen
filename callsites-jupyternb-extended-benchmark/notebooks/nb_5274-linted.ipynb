{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db875102",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "source_hidden": true
   },
   "source": [
    "### Index of ML Operations<a id='top_phases'></a>\n",
    "<div><ul>\n",
    "<ul><li><details><summary style='list-style: none; cursor: pointer;'><strong>Imported Libraries</strong></summary>\n",
    "<ul>\n",
    "\n",
    "<li><b>matplotlib</b></li>\n",
    "<li><b>sklearn</b></li>\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<ul><li><details><summary style='list-style: none; cursor: pointer;'><strong>Visualization</strong></summary>\n",
    "<ul>\n",
    "\n",
    "<li><details><summary style='list-style: none; cursor: pointer;'><u>View All \"Visualization\" Calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <b>matplotlib</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot.scatter</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x, y : float or array-like, shape (n, )\n",
    "    The data positions.\n",
    "\n",
    "s : float or array-like, shape (n, ), optional\n",
    "    The marker size in points**2 (typographic points are 1/72 in.).\n",
    "    Default is ``rcParams['lines.markersize'] ** 2``.\n",
    "\n",
    "    The linewidth and edgecolor can visually interact with the marker\n",
    "    size, and can lead to artifacts if the marker size is smaller than\n",
    "    the linewidth.\n",
    "\n",
    "    If the linewidth is greater than 0 and the edgecolor is anything\n",
    "    but *'none'*, then the effective size of the marker will be\n",
    "    increased by half the linewidth because the stroke will be centered\n",
    "    on the edge of the shape.\n",
    "\n",
    "    To eliminate the marker edge either set *linewidth=0* or\n",
    "    *edgecolor='none'*.\n",
    "\n",
    "c : array-like or list of colors or color, optional\n",
    "    The marker colors. Possible values:\n",
    "\n",
    "    - A scalar or sequence of n numbers to be mapped to colors using\n",
    "      *cmap* and *norm*.\n",
    "    - A 2D array in which the rows are RGB or RGBA.\n",
    "    - A sequence of colors of length n.\n",
    "    - A single color format string.\n",
    "\n",
    "    Note that *c* should not be a single numeric RGB or RGBA sequence\n",
    "    because that is indistinguishable from an array of values to be\n",
    "    colormapped. If you want to specify the same RGB or RGBA value for\n",
    "    all points, use a 2D array with a single row.  Otherwise,\n",
    "    value-matching will have precedence in case of a size matching with\n",
    "    *x* and *y*.\n",
    "\n",
    "    If you wish to specify a single color for all points\n",
    "    prefer the *color* keyword argument.\n",
    "\n",
    "    Defaults to `None`. In that case the marker color is determined\n",
    "    by the value of *color*, *facecolor* or *facecolors*. In case\n",
    "    those are not specified or `None`, the marker color is determined\n",
    "    by the next color of the ``Axes``' current \"shape and fill\" color\n",
    "    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n",
    "\n",
    "marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n",
    "    The marker style. *marker* can be either an instance of the class\n",
    "    or the text shorthand for a particular marker.\n",
    "    See :mod:`matplotlib.markers` for more information about marker\n",
    "    styles.\n",
    "\n",
    "cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
    "    The Colormap instance or registered colormap name used to map scalar data\n",
    "    to colors.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "norm : str or `~matplotlib.colors.Normalize`, optional\n",
    "    The normalization method used to scale scalar data to the [0, 1] range\n",
    "    before mapping to colors using *cmap*. By default, a linear scaling is\n",
    "    used, mapping the lowest value to 0 and the highest to 1.\n",
    "\n",
    "    If given, this can be one of the following:\n",
    "\n",
    "    - An instance of `.Normalize` or one of its subclasses\n",
    "      (see :ref:`colormapnorms`).\n",
    "    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n",
    "      list of available scales, call `matplotlib.scale.get_scale_names()`.\n",
    "      In that case, a suitable `.Normalize` subclass is dynamically generated\n",
    "      and instantiated.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "vmin, vmax : float, optional\n",
    "    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n",
    "    the data range that the colormap covers. By default, the colormap covers\n",
    "    the complete value range of the supplied data. It is an error to use\n",
    "    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n",
    "    name together with *vmin*/*vmax* is acceptable).\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "alpha : float, default: None\n",
    "    The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "\n",
    "linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
    "    The linewidth of the marker edges. Note: The default *edgecolors*\n",
    "    is 'face'. You may want to change this as well.\n",
    "\n",
    "edgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n",
    "    The edge color of the marker. Possible values:\n",
    "\n",
    "    - 'face': The edge color will always be the same as the face color.\n",
    "    - 'none': No patch boundary will be drawn.\n",
    "    - A color or sequence of colors.\n",
    "\n",
    "    For non-filled markers, *edgecolors* is ignored. Instead, the color\n",
    "    is determined like with 'face', i.e. from *c*, *colors*, or\n",
    "    *facecolors*.\n",
    "\n",
    "plotnonfinite : bool, default: False\n",
    "    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n",
    "    or ``nan``). If ``True`` the points are drawn with the *bad*\n",
    "    colormap color (see `.Colormap.set_bad`).\n",
    "\n",
    "Returns\n",
    "-------\n",
    "`~matplotlib.collections.PathCollection`\n",
    "\n",
    "Other Parameters\n",
    "----------------\n",
    "data : indexable object, optional\n",
    "    If given, the following parameters also accept a string ``s``, which is\n",
    "    interpreted as ``data[s]`` (unless this raises an exception):\n",
    "\n",
    "    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n",
    "**kwargs : `~matplotlib.collections.Collection` properties\n",
    "\n",
    "See Also\n",
    "--------\n",
    "plot : To plot scatter plots when markers are identical in size and\n",
    "    color.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "* The `.plot` function will be faster for scatterplots where markers\n",
    "  don't vary in size or color.\n",
    "\n",
    "* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n",
    "  case all masks will be combined and only unmasked points will be\n",
    "  plotted.\n",
    "\n",
    "* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n",
    "  may be input as N-D arrays, but within scatter they will be\n",
    "  flattened. The exception is *c*, which will be flattened only if its\n",
    "  size matches the size of *x* and *y*.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot.plot</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Plot y versus x as lines and/or markers.\n",
    "\n",
    "Call signatures::\n",
    "\n",
    "    plot([x], y, [fmt], *, data=None, **kwargs)\n",
    "    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
    "\n",
    "The coordinates of the points or line nodes are given by *x*, *y*.\n",
    "\n",
    "The optional parameter *fmt* is a convenient way for defining basic\n",
    "formatting like color, marker and linestyle. It's a shortcut string\n",
    "notation described in the *Notes* section below.\n",
    "\n",
    ">>> plot(x, y)        # plot x and y using default line style and color\n",
    ">>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n",
    ">>> plot(y)           # plot y using x as index array 0..N-1\n",
    ">>> plot(y, 'r+')     # ditto, but with red plusses\n",
    "\n",
    "You can use `.Line2D` properties as keyword arguments for more\n",
    "control on the appearance. Line properties and *fmt* can be mixed.\n",
    "The following two calls yield identical results:\n",
    "\n",
    ">>> plot(x, y, 'go--', linewidth=2, markersize=12)\n",
    ">>> plot(x, y, color='green', marker='o', linestyle='dashed',\n",
    "...      linewidth=2, markersize=12)\n",
    "\n",
    "When conflicting with *fmt*, keyword arguments take precedence.\n",
    "\n",
    "\n",
    "**Plotting labelled data**\n",
    "\n",
    "There's a convenient way for plotting objects with labelled data (i.e.\n",
    "data that can be accessed by index ``obj['y']``). Instead of giving\n",
    "the data in *x* and *y*, you can provide the object in the *data*\n",
    "parameter and just give the labels for *x* and *y*::\n",
    "\n",
    ">>> plot('xlabel', 'ylabel', data=obj)\n",
    "\n",
    "All indexable objects are supported. This could e.g. be a `dict`, a\n",
    "`pandas.DataFrame` or a structured numpy array.\n",
    "\n",
    "\n",
    "**Plotting multiple sets of data**\n",
    "\n",
    "There are various ways to plot multiple sets of data.\n",
    "\n",
    "- The most straight forward way is just to call `plot` multiple times.\n",
    "  Example:\n",
    "\n",
    "  >>> plot(x1, y1, 'bo')\n",
    "  >>> plot(x2, y2, 'go')\n",
    "\n",
    "- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n",
    "  for every column. If both *x* and *y* are 2D, they must have the\n",
    "  same shape. If only one of them is 2D with shape (N, m) the other\n",
    "  must have length N and will be used for every data set m.\n",
    "\n",
    "  Example:\n",
    "\n",
    "  >>> x = [1, 2, 3]\n",
    "  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "  >>> plot(x, y)\n",
    "\n",
    "  is equivalent to:\n",
    "\n",
    "  >>> for col in range(y.shape[1]):\n",
    "  ...     plot(x, y[:, col])\n",
    "\n",
    "- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n",
    "  groups::\n",
    "\n",
    "  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n",
    "\n",
    "  In this case, any additional keyword argument applies to all\n",
    "  datasets. Also, this syntax cannot be combined with the *data*\n",
    "  parameter.\n",
    "\n",
    "By default, each line is assigned a different style specified by a\n",
    "'style cycle'. The *fmt* and line property parameters are only\n",
    "necessary if you want explicit deviations from these defaults.\n",
    "Alternatively, you can also change the style cycle using\n",
    ":rc:`axes.prop_cycle`.\n",
    "\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x, y : array-like or scalar\n",
    "    The horizontal / vertical coordinates of the data points.\n",
    "    *x* values are optional and default to ``range(len(y))``.\n",
    "\n",
    "    Commonly, these parameters are 1D arrays.\n",
    "\n",
    "    They can also be scalars, or two-dimensional (in that case, the\n",
    "    columns represent separate data sets).\n",
    "\n",
    "    These arguments cannot be passed as keywords.\n",
    "\n",
    "fmt : str, optional\n",
    "    A format string, e.g. 'ro' for red circles. See the *Notes*\n",
    "    section for a full description of the format strings.\n",
    "\n",
    "    Format strings are just an abbreviation for quickly setting\n",
    "    basic line properties. All of these and more can also be\n",
    "    controlled by keyword arguments.\n",
    "\n",
    "    This argument cannot be passed as keyword.\n",
    "\n",
    "data : indexable object, optional\n",
    "    An object with labelled data. If given, provide the label names to\n",
    "    plot in *x* and *y*.\n",
    "\n",
    "    .. note::\n",
    "        Technically there's a slight ambiguity in calls where the\n",
    "        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n",
    "        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n",
    "        the former interpretation is chosen, but a warning is issued.\n",
    "        You may suppress the warning by adding an empty format string\n",
    "        ``plot('n', 'o', '', data=obj)``.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "list of `.Line2D`\n",
    "    A list of lines representing the plotted data.\n",
    "\n",
    "Other Parameters\n",
    "----------------\n",
    "scalex, scaley : bool, default: True\n",
    "    These parameters determine if the view limits are adapted to the\n",
    "    data limits. The values are passed on to\n",
    "    `~.axes.Axes.autoscale_view`.\n",
    "\n",
    "**kwargs : `~matplotlib.lines.Line2D` properties, optional\n",
    "    *kwargs* are used to specify properties like a line label (for\n",
    "    auto legends), linewidth, antialiasing, marker face color.\n",
    "    Example::\n",
    "\n",
    "    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n",
    "    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n",
    "\n",
    "    If you specify multiple lines with one plot call, the kwargs apply\n",
    "    to all those lines. In case the label object is iterable, each\n",
    "    element is used as labels for each set of data.\n",
    "\n",
    "    Here is a list of available `.Line2D` properties:\n",
    "\n",
    "    Properties:\n",
    "    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n",
    "    alpha: scalar or None\n",
    "    animated: bool\n",
    "    antialiased or aa: bool\n",
    "    clip_box: `~matplotlib.transforms.BboxBase` or None\n",
    "    clip_on: bool\n",
    "    clip_path: Patch or (Path, Transform) or None\n",
    "    color or c: color\n",
    "    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n",
    "    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n",
    "    dashes: sequence of floats (on/off ink in points) or (None, None)\n",
    "    data: (2, N) array or two 1D arrays\n",
    "    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
    "    figure: `~matplotlib.figure.Figure`\n",
    "    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
    "    gapcolor: color or None\n",
    "    gid: str\n",
    "    in_layout: bool\n",
    "    label: object\n",
    "    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
    "    linewidth or lw: float\n",
    "    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
    "    markeredgecolor or mec: color\n",
    "    markeredgewidth or mew: float\n",
    "    markerfacecolor or mfc: color\n",
    "    markerfacecoloralt or mfcalt: color\n",
    "    markersize or ms: float\n",
    "    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n",
    "    mouseover: bool\n",
    "    path_effects: list of `.AbstractPathEffect`\n",
    "    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n",
    "    pickradius: float\n",
    "    rasterized: bool\n",
    "    sketch_params: (scale: float, length: float, randomness: float)\n",
    "    snap: bool or None\n",
    "    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n",
    "    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n",
    "    transform: unknown\n",
    "    url: str\n",
    "    visible: bool\n",
    "    xdata: 1D array\n",
    "    ydata: 1D array\n",
    "    zorder: float\n",
    "\n",
    "See Also\n",
    "--------\n",
    "scatter : XY scatter plot with markers of varying size and/or color (\n",
    "    sometimes also called bubble chart).\n",
    "\n",
    "Notes\n",
    "-----\n",
    "**Format Strings**\n",
    "\n",
    "A format string consists of a part for color, marker and line::\n",
    "\n",
    "    fmt = '[marker][line][color]'\n",
    "\n",
    "Each of them is optional. If not provided, the value from the style\n",
    "cycle is used. Exception: If ``line`` is given, but no ``marker``,\n",
    "the data will be a line without markers.\n",
    "\n",
    "Other combinations such as ``[color][marker][line]`` are also\n",
    "supported, but note that their parsing may be ambiguous.\n",
    "\n",
    "**Markers**\n",
    "\n",
    "=============   ===============================\n",
    "character       description\n",
    "=============   ===============================\n",
    "``'.'``         point marker\n",
    "``','``         pixel marker\n",
    "``'o'``         circle marker\n",
    "``'v'``         triangle_down marker\n",
    "``'^'``         triangle_up marker\n",
    "``'<'``         triangle_left marker\n",
    "``'>'``         triangle_right marker\n",
    "``'1'``         tri_down marker\n",
    "``'2'``         tri_up marker\n",
    "``'3'``         tri_left marker\n",
    "``'4'``         tri_right marker\n",
    "``'8'``         octagon marker\n",
    "``'s'``         square marker\n",
    "``'p'``         pentagon marker\n",
    "``'P'``         plus (filled) marker\n",
    "``'*'``         star marker\n",
    "``'h'``         hexagon1 marker\n",
    "``'H'``         hexagon2 marker\n",
    "``'+'``         plus marker\n",
    "``'x'``         x marker\n",
    "``'X'``         x (filled) marker\n",
    "``'D'``         diamond marker\n",
    "``'d'``         thin_diamond marker\n",
    "``'|'``         vline marker\n",
    "``'_'``         hline marker\n",
    "=============   ===============================\n",
    "\n",
    "**Line Styles**\n",
    "\n",
    "=============    ===============================\n",
    "character        description\n",
    "=============    ===============================\n",
    "``'-'``          solid line style\n",
    "``'--'``         dashed line style\n",
    "``'-.'``         dash-dot line style\n",
    "``':'``          dotted line style\n",
    "=============    ===============================\n",
    "\n",
    "Example format strings::\n",
    "\n",
    "    'b'    # blue markers with default shape\n",
    "    'or'   # red circles\n",
    "    '-g'   # green solid line\n",
    "    '--'   # dashed line with default color\n",
    "    '^k:'  # black triangle_up markers connected by a dotted line\n",
    "\n",
    "**Colors**\n",
    "\n",
    "The supported color abbreviations are the single letter codes\n",
    "\n",
    "=============    ===============================\n",
    "character        color\n",
    "=============    ===============================\n",
    "``'b'``          blue\n",
    "``'g'``          green\n",
    "``'r'``          red\n",
    "``'c'``          cyan\n",
    "``'m'``          magenta\n",
    "``'y'``          yellow\n",
    "``'k'``          black\n",
    "``'w'``          white\n",
    "=============    ===============================\n",
    "\n",
    "and the ``'CN'`` colors that index into the default property cycle.\n",
    "\n",
    "If the color is the only part of the format string, you can\n",
    "additionally use any  `matplotlib.colors` spec, e.g. full names\n",
    "(``'green'``) or hex strings (``'#008000'``).\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 7</u></strong></summary><small><a href=#7>goto cell # 7</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>matplotlib</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot.scatter</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x, y : float or array-like, shape (n, )\n",
    "    The data positions.\n",
    "\n",
    "s : float or array-like, shape (n, ), optional\n",
    "    The marker size in points**2 (typographic points are 1/72 in.).\n",
    "    Default is ``rcParams['lines.markersize'] ** 2``.\n",
    "\n",
    "    The linewidth and edgecolor can visually interact with the marker\n",
    "    size, and can lead to artifacts if the marker size is smaller than\n",
    "    the linewidth.\n",
    "\n",
    "    If the linewidth is greater than 0 and the edgecolor is anything\n",
    "    but *'none'*, then the effective size of the marker will be\n",
    "    increased by half the linewidth because the stroke will be centered\n",
    "    on the edge of the shape.\n",
    "\n",
    "    To eliminate the marker edge either set *linewidth=0* or\n",
    "    *edgecolor='none'*.\n",
    "\n",
    "c : array-like or list of colors or color, optional\n",
    "    The marker colors. Possible values:\n",
    "\n",
    "    - A scalar or sequence of n numbers to be mapped to colors using\n",
    "      *cmap* and *norm*.\n",
    "    - A 2D array in which the rows are RGB or RGBA.\n",
    "    - A sequence of colors of length n.\n",
    "    - A single color format string.\n",
    "\n",
    "    Note that *c* should not be a single numeric RGB or RGBA sequence\n",
    "    because that is indistinguishable from an array of values to be\n",
    "    colormapped. If you want to specify the same RGB or RGBA value for\n",
    "    all points, use a 2D array with a single row.  Otherwise,\n",
    "    value-matching will have precedence in case of a size matching with\n",
    "    *x* and *y*.\n",
    "\n",
    "    If you wish to specify a single color for all points\n",
    "    prefer the *color* keyword argument.\n",
    "\n",
    "    Defaults to `None`. In that case the marker color is determined\n",
    "    by the value of *color*, *facecolor* or *facecolors*. In case\n",
    "    those are not specified or `None`, the marker color is determined\n",
    "    by the next color of the ``Axes``' current \"shape and fill\" color\n",
    "    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n",
    "\n",
    "marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n",
    "    The marker style. *marker* can be either an instance of the class\n",
    "    or the text shorthand for a particular marker.\n",
    "    See :mod:`matplotlib.markers` for more information about marker\n",
    "    styles.\n",
    "\n",
    "cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
    "    The Colormap instance or registered colormap name used to map scalar data\n",
    "    to colors.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "norm : str or `~matplotlib.colors.Normalize`, optional\n",
    "    The normalization method used to scale scalar data to the [0, 1] range\n",
    "    before mapping to colors using *cmap*. By default, a linear scaling is\n",
    "    used, mapping the lowest value to 0 and the highest to 1.\n",
    "\n",
    "    If given, this can be one of the following:\n",
    "\n",
    "    - An instance of `.Normalize` or one of its subclasses\n",
    "      (see :ref:`colormapnorms`).\n",
    "    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n",
    "      list of available scales, call `matplotlib.scale.get_scale_names()`.\n",
    "      In that case, a suitable `.Normalize` subclass is dynamically generated\n",
    "      and instantiated.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "vmin, vmax : float, optional\n",
    "    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n",
    "    the data range that the colormap covers. By default, the colormap covers\n",
    "    the complete value range of the supplied data. It is an error to use\n",
    "    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n",
    "    name together with *vmin*/*vmax* is acceptable).\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "alpha : float, default: None\n",
    "    The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "\n",
    "linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
    "    The linewidth of the marker edges. Note: The default *edgecolors*\n",
    "    is 'face'. You may want to change this as well.\n",
    "\n",
    "edgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n",
    "    The edge color of the marker. Possible values:\n",
    "\n",
    "    - 'face': The edge color will always be the same as the face color.\n",
    "    - 'none': No patch boundary will be drawn.\n",
    "    - A color or sequence of colors.\n",
    "\n",
    "    For non-filled markers, *edgecolors* is ignored. Instead, the color\n",
    "    is determined like with 'face', i.e. from *c*, *colors*, or\n",
    "    *facecolors*.\n",
    "\n",
    "plotnonfinite : bool, default: False\n",
    "    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n",
    "    or ``nan``). If ``True`` the points are drawn with the *bad*\n",
    "    colormap color (see `.Colormap.set_bad`).\n",
    "\n",
    "Returns\n",
    "-------\n",
    "`~matplotlib.collections.PathCollection`\n",
    "\n",
    "Other Parameters\n",
    "----------------\n",
    "data : indexable object, optional\n",
    "    If given, the following parameters also accept a string ``s``, which is\n",
    "    interpreted as ``data[s]`` (unless this raises an exception):\n",
    "\n",
    "    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n",
    "**kwargs : `~matplotlib.collections.Collection` properties\n",
    "\n",
    "See Also\n",
    "--------\n",
    "plot : To plot scatter plots when markers are identical in size and\n",
    "    color.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "* The `.plot` function will be faster for scatterplots where markers\n",
    "  don't vary in size or color.\n",
    "\n",
    "* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n",
    "  case all masks will be combined and only unmasked points will be\n",
    "  plotted.\n",
    "\n",
    "* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n",
    "  may be input as N-D arrays, but within scatter they will be\n",
    "  flattened. The exception is *c*, which will be flattened only if its\n",
    "  size matches the size of *x* and *y*.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 10</u></strong></summary><small><a href=#10>goto cell # 10</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>matplotlib</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot.plot</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Plot y versus x as lines and/or markers.\n",
    "\n",
    "Call signatures::\n",
    "\n",
    "    plot([x], y, [fmt], *, data=None, **kwargs)\n",
    "    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
    "\n",
    "The coordinates of the points or line nodes are given by *x*, *y*.\n",
    "\n",
    "The optional parameter *fmt* is a convenient way for defining basic\n",
    "formatting like color, marker and linestyle. It's a shortcut string\n",
    "notation described in the *Notes* section below.\n",
    "\n",
    ">>> plot(x, y)        # plot x and y using default line style and color\n",
    ">>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n",
    ">>> plot(y)           # plot y using x as index array 0..N-1\n",
    ">>> plot(y, 'r+')     # ditto, but with red plusses\n",
    "\n",
    "You can use `.Line2D` properties as keyword arguments for more\n",
    "control on the appearance. Line properties and *fmt* can be mixed.\n",
    "The following two calls yield identical results:\n",
    "\n",
    ">>> plot(x, y, 'go--', linewidth=2, markersize=12)\n",
    ">>> plot(x, y, color='green', marker='o', linestyle='dashed',\n",
    "...      linewidth=2, markersize=12)\n",
    "\n",
    "When conflicting with *fmt*, keyword arguments take precedence.\n",
    "\n",
    "\n",
    "**Plotting labelled data**\n",
    "\n",
    "There's a convenient way for plotting objects with labelled data (i.e.\n",
    "data that can be accessed by index ``obj['y']``). Instead of giving\n",
    "the data in *x* and *y*, you can provide the object in the *data*\n",
    "parameter and just give the labels for *x* and *y*::\n",
    "\n",
    ">>> plot('xlabel', 'ylabel', data=obj)\n",
    "\n",
    "All indexable objects are supported. This could e.g. be a `dict`, a\n",
    "`pandas.DataFrame` or a structured numpy array.\n",
    "\n",
    "\n",
    "**Plotting multiple sets of data**\n",
    "\n",
    "There are various ways to plot multiple sets of data.\n",
    "\n",
    "- The most straight forward way is just to call `plot` multiple times.\n",
    "  Example:\n",
    "\n",
    "  >>> plot(x1, y1, 'bo')\n",
    "  >>> plot(x2, y2, 'go')\n",
    "\n",
    "- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n",
    "  for every column. If both *x* and *y* are 2D, they must have the\n",
    "  same shape. If only one of them is 2D with shape (N, m) the other\n",
    "  must have length N and will be used for every data set m.\n",
    "\n",
    "  Example:\n",
    "\n",
    "  >>> x = [1, 2, 3]\n",
    "  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "  >>> plot(x, y)\n",
    "\n",
    "  is equivalent to:\n",
    "\n",
    "  >>> for col in range(y.shape[1]):\n",
    "  ...     plot(x, y[:, col])\n",
    "\n",
    "- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n",
    "  groups::\n",
    "\n",
    "  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n",
    "\n",
    "  In this case, any additional keyword argument applies to all\n",
    "  datasets. Also, this syntax cannot be combined with the *data*\n",
    "  parameter.\n",
    "\n",
    "By default, each line is assigned a different style specified by a\n",
    "'style cycle'. The *fmt* and line property parameters are only\n",
    "necessary if you want explicit deviations from these defaults.\n",
    "Alternatively, you can also change the style cycle using\n",
    ":rc:`axes.prop_cycle`.\n",
    "\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x, y : array-like or scalar\n",
    "    The horizontal / vertical coordinates of the data points.\n",
    "    *x* values are optional and default to ``range(len(y))``.\n",
    "\n",
    "    Commonly, these parameters are 1D arrays.\n",
    "\n",
    "    They can also be scalars, or two-dimensional (in that case, the\n",
    "    columns represent separate data sets).\n",
    "\n",
    "    These arguments cannot be passed as keywords.\n",
    "\n",
    "fmt : str, optional\n",
    "    A format string, e.g. 'ro' for red circles. See the *Notes*\n",
    "    section for a full description of the format strings.\n",
    "\n",
    "    Format strings are just an abbreviation for quickly setting\n",
    "    basic line properties. All of these and more can also be\n",
    "    controlled by keyword arguments.\n",
    "\n",
    "    This argument cannot be passed as keyword.\n",
    "\n",
    "data : indexable object, optional\n",
    "    An object with labelled data. If given, provide the label names to\n",
    "    plot in *x* and *y*.\n",
    "\n",
    "    .. note::\n",
    "        Technically there's a slight ambiguity in calls where the\n",
    "        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n",
    "        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n",
    "        the former interpretation is chosen, but a warning is issued.\n",
    "        You may suppress the warning by adding an empty format string\n",
    "        ``plot('n', 'o', '', data=obj)``.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "list of `.Line2D`\n",
    "    A list of lines representing the plotted data.\n",
    "\n",
    "Other Parameters\n",
    "----------------\n",
    "scalex, scaley : bool, default: True\n",
    "    These parameters determine if the view limits are adapted to the\n",
    "    data limits. The values are passed on to\n",
    "    `~.axes.Axes.autoscale_view`.\n",
    "\n",
    "**kwargs : `~matplotlib.lines.Line2D` properties, optional\n",
    "    *kwargs* are used to specify properties like a line label (for\n",
    "    auto legends), linewidth, antialiasing, marker face color.\n",
    "    Example::\n",
    "\n",
    "    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n",
    "    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n",
    "\n",
    "    If you specify multiple lines with one plot call, the kwargs apply\n",
    "    to all those lines. In case the label object is iterable, each\n",
    "    element is used as labels for each set of data.\n",
    "\n",
    "    Here is a list of available `.Line2D` properties:\n",
    "\n",
    "    Properties:\n",
    "    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n",
    "    alpha: scalar or None\n",
    "    animated: bool\n",
    "    antialiased or aa: bool\n",
    "    clip_box: `~matplotlib.transforms.BboxBase` or None\n",
    "    clip_on: bool\n",
    "    clip_path: Patch or (Path, Transform) or None\n",
    "    color or c: color\n",
    "    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n",
    "    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n",
    "    dashes: sequence of floats (on/off ink in points) or (None, None)\n",
    "    data: (2, N) array or two 1D arrays\n",
    "    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
    "    figure: `~matplotlib.figure.Figure`\n",
    "    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
    "    gapcolor: color or None\n",
    "    gid: str\n",
    "    in_layout: bool\n",
    "    label: object\n",
    "    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
    "    linewidth or lw: float\n",
    "    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
    "    markeredgecolor or mec: color\n",
    "    markeredgewidth or mew: float\n",
    "    markerfacecolor or mfc: color\n",
    "    markerfacecoloralt or mfcalt: color\n",
    "    markersize or ms: float\n",
    "    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n",
    "    mouseover: bool\n",
    "    path_effects: list of `.AbstractPathEffect`\n",
    "    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n",
    "    pickradius: float\n",
    "    rasterized: bool\n",
    "    sketch_params: (scale: float, length: float, randomness: float)\n",
    "    snap: bool or None\n",
    "    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n",
    "    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n",
    "    transform: unknown\n",
    "    url: str\n",
    "    visible: bool\n",
    "    xdata: 1D array\n",
    "    ydata: 1D array\n",
    "    zorder: float\n",
    "\n",
    "See Also\n",
    "--------\n",
    "scatter : XY scatter plot with markers of varying size and/or color (\n",
    "    sometimes also called bubble chart).\n",
    "\n",
    "Notes\n",
    "-----\n",
    "**Format Strings**\n",
    "\n",
    "A format string consists of a part for color, marker and line::\n",
    "\n",
    "    fmt = '[marker][line][color]'\n",
    "\n",
    "Each of them is optional. If not provided, the value from the style\n",
    "cycle is used. Exception: If ``line`` is given, but no ``marker``,\n",
    "the data will be a line without markers.\n",
    "\n",
    "Other combinations such as ``[color][marker][line]`` are also\n",
    "supported, but note that their parsing may be ambiguous.\n",
    "\n",
    "**Markers**\n",
    "\n",
    "=============   ===============================\n",
    "character       description\n",
    "=============   ===============================\n",
    "``'.'``         point marker\n",
    "``','``         pixel marker\n",
    "``'o'``         circle marker\n",
    "``'v'``         triangle_down marker\n",
    "``'^'``         triangle_up marker\n",
    "``'<'``         triangle_left marker\n",
    "``'>'``         triangle_right marker\n",
    "``'1'``         tri_down marker\n",
    "``'2'``         tri_up marker\n",
    "``'3'``         tri_left marker\n",
    "``'4'``         tri_right marker\n",
    "``'8'``         octagon marker\n",
    "``'s'``         square marker\n",
    "``'p'``         pentagon marker\n",
    "``'P'``         plus (filled) marker\n",
    "``'*'``         star marker\n",
    "``'h'``         hexagon1 marker\n",
    "``'H'``         hexagon2 marker\n",
    "``'+'``         plus marker\n",
    "``'x'``         x marker\n",
    "``'X'``         x (filled) marker\n",
    "``'D'``         diamond marker\n",
    "``'d'``         thin_diamond marker\n",
    "``'|'``         vline marker\n",
    "``'_'``         hline marker\n",
    "=============   ===============================\n",
    "\n",
    "**Line Styles**\n",
    "\n",
    "=============    ===============================\n",
    "character        description\n",
    "=============    ===============================\n",
    "``'-'``          solid line style\n",
    "``'--'``         dashed line style\n",
    "``'-.'``         dash-dot line style\n",
    "``':'``          dotted line style\n",
    "=============    ===============================\n",
    "\n",
    "Example format strings::\n",
    "\n",
    "    'b'    # blue markers with default shape\n",
    "    'or'   # red circles\n",
    "    '-g'   # green solid line\n",
    "    '--'   # dashed line with default color\n",
    "    '^k:'  # black triangle_up markers connected by a dotted line\n",
    "\n",
    "**Colors**\n",
    "\n",
    "The supported color abbreviations are the single letter codes\n",
    "\n",
    "=============    ===============================\n",
    "character        color\n",
    "=============    ===============================\n",
    "``'b'``          blue\n",
    "``'g'``          green\n",
    "``'r'``          red\n",
    "``'c'``          cyan\n",
    "``'m'``          magenta\n",
    "``'y'``          yellow\n",
    "``'k'``          black\n",
    "``'w'``          white\n",
    "=============    ===============================\n",
    "\n",
    "and the ``'CN'`` colors that index into the default property cycle.\n",
    "\n",
    "If the color is the only part of the format string, you can\n",
    "additionally use any  `matplotlib.colors` spec, e.g. full names\n",
    "(``'green'``) or hex strings (``'#008000'``).\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 15</u></strong></summary><small><a href=#15>goto cell # 15</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>matplotlib</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot.scatter</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x, y : float or array-like, shape (n, )\n",
    "    The data positions.\n",
    "\n",
    "s : float or array-like, shape (n, ), optional\n",
    "    The marker size in points**2 (typographic points are 1/72 in.).\n",
    "    Default is ``rcParams['lines.markersize'] ** 2``.\n",
    "\n",
    "    The linewidth and edgecolor can visually interact with the marker\n",
    "    size, and can lead to artifacts if the marker size is smaller than\n",
    "    the linewidth.\n",
    "\n",
    "    If the linewidth is greater than 0 and the edgecolor is anything\n",
    "    but *'none'*, then the effective size of the marker will be\n",
    "    increased by half the linewidth because the stroke will be centered\n",
    "    on the edge of the shape.\n",
    "\n",
    "    To eliminate the marker edge either set *linewidth=0* or\n",
    "    *edgecolor='none'*.\n",
    "\n",
    "c : array-like or list of colors or color, optional\n",
    "    The marker colors. Possible values:\n",
    "\n",
    "    - A scalar or sequence of n numbers to be mapped to colors using\n",
    "      *cmap* and *norm*.\n",
    "    - A 2D array in which the rows are RGB or RGBA.\n",
    "    - A sequence of colors of length n.\n",
    "    - A single color format string.\n",
    "\n",
    "    Note that *c* should not be a single numeric RGB or RGBA sequence\n",
    "    because that is indistinguishable from an array of values to be\n",
    "    colormapped. If you want to specify the same RGB or RGBA value for\n",
    "    all points, use a 2D array with a single row.  Otherwise,\n",
    "    value-matching will have precedence in case of a size matching with\n",
    "    *x* and *y*.\n",
    "\n",
    "    If you wish to specify a single color for all points\n",
    "    prefer the *color* keyword argument.\n",
    "\n",
    "    Defaults to `None`. In that case the marker color is determined\n",
    "    by the value of *color*, *facecolor* or *facecolors*. In case\n",
    "    those are not specified or `None`, the marker color is determined\n",
    "    by the next color of the ``Axes``' current \"shape and fill\" color\n",
    "    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n",
    "\n",
    "marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n",
    "    The marker style. *marker* can be either an instance of the class\n",
    "    or the text shorthand for a particular marker.\n",
    "    See :mod:`matplotlib.markers` for more information about marker\n",
    "    styles.\n",
    "\n",
    "cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
    "    The Colormap instance or registered colormap name used to map scalar data\n",
    "    to colors.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "norm : str or `~matplotlib.colors.Normalize`, optional\n",
    "    The normalization method used to scale scalar data to the [0, 1] range\n",
    "    before mapping to colors using *cmap*. By default, a linear scaling is\n",
    "    used, mapping the lowest value to 0 and the highest to 1.\n",
    "\n",
    "    If given, this can be one of the following:\n",
    "\n",
    "    - An instance of `.Normalize` or one of its subclasses\n",
    "      (see :ref:`colormapnorms`).\n",
    "    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n",
    "      list of available scales, call `matplotlib.scale.get_scale_names()`.\n",
    "      In that case, a suitable `.Normalize` subclass is dynamically generated\n",
    "      and instantiated.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "vmin, vmax : float, optional\n",
    "    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n",
    "    the data range that the colormap covers. By default, the colormap covers\n",
    "    the complete value range of the supplied data. It is an error to use\n",
    "    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n",
    "    name together with *vmin*/*vmax* is acceptable).\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "alpha : float, default: None\n",
    "    The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "\n",
    "linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
    "    The linewidth of the marker edges. Note: The default *edgecolors*\n",
    "    is 'face'. You may want to change this as well.\n",
    "\n",
    "edgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n",
    "    The edge color of the marker. Possible values:\n",
    "\n",
    "    - 'face': The edge color will always be the same as the face color.\n",
    "    - 'none': No patch boundary will be drawn.\n",
    "    - A color or sequence of colors.\n",
    "\n",
    "    For non-filled markers, *edgecolors* is ignored. Instead, the color\n",
    "    is determined like with 'face', i.e. from *c*, *colors*, or\n",
    "    *facecolors*.\n",
    "\n",
    "plotnonfinite : bool, default: False\n",
    "    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n",
    "    or ``nan``). If ``True`` the points are drawn with the *bad*\n",
    "    colormap color (see `.Colormap.set_bad`).\n",
    "\n",
    "Returns\n",
    "-------\n",
    "`~matplotlib.collections.PathCollection`\n",
    "\n",
    "Other Parameters\n",
    "----------------\n",
    "data : indexable object, optional\n",
    "    If given, the following parameters also accept a string ``s``, which is\n",
    "    interpreted as ``data[s]`` (unless this raises an exception):\n",
    "\n",
    "    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n",
    "**kwargs : `~matplotlib.collections.Collection` properties\n",
    "\n",
    "See Also\n",
    "--------\n",
    "plot : To plot scatter plots when markers are identical in size and\n",
    "    color.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "* The `.plot` function will be faster for scatterplots where markers\n",
    "  don't vary in size or color.\n",
    "\n",
    "* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n",
    "  case all masks will be combined and only unmasked points will be\n",
    "  plotted.\n",
    "\n",
    "* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n",
    "  may be input as N-D arrays, but within scatter they will be\n",
    "  flattened. The exception is *c*, which will be flattened only if its\n",
    "  size matches the size of *x* and *y*.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 20</u></strong></summary><small><a href=#20>goto cell # 20</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>matplotlib</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot.scatter</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x, y : float or array-like, shape (n, )\n",
    "    The data positions.\n",
    "\n",
    "s : float or array-like, shape (n, ), optional\n",
    "    The marker size in points**2 (typographic points are 1/72 in.).\n",
    "    Default is ``rcParams['lines.markersize'] ** 2``.\n",
    "\n",
    "    The linewidth and edgecolor can visually interact with the marker\n",
    "    size, and can lead to artifacts if the marker size is smaller than\n",
    "    the linewidth.\n",
    "\n",
    "    If the linewidth is greater than 0 and the edgecolor is anything\n",
    "    but *'none'*, then the effective size of the marker will be\n",
    "    increased by half the linewidth because the stroke will be centered\n",
    "    on the edge of the shape.\n",
    "\n",
    "    To eliminate the marker edge either set *linewidth=0* or\n",
    "    *edgecolor='none'*.\n",
    "\n",
    "c : array-like or list of colors or color, optional\n",
    "    The marker colors. Possible values:\n",
    "\n",
    "    - A scalar or sequence of n numbers to be mapped to colors using\n",
    "      *cmap* and *norm*.\n",
    "    - A 2D array in which the rows are RGB or RGBA.\n",
    "    - A sequence of colors of length n.\n",
    "    - A single color format string.\n",
    "\n",
    "    Note that *c* should not be a single numeric RGB or RGBA sequence\n",
    "    because that is indistinguishable from an array of values to be\n",
    "    colormapped. If you want to specify the same RGB or RGBA value for\n",
    "    all points, use a 2D array with a single row.  Otherwise,\n",
    "    value-matching will have precedence in case of a size matching with\n",
    "    *x* and *y*.\n",
    "\n",
    "    If you wish to specify a single color for all points\n",
    "    prefer the *color* keyword argument.\n",
    "\n",
    "    Defaults to `None`. In that case the marker color is determined\n",
    "    by the value of *color*, *facecolor* or *facecolors*. In case\n",
    "    those are not specified or `None`, the marker color is determined\n",
    "    by the next color of the ``Axes``' current \"shape and fill\" color\n",
    "    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n",
    "\n",
    "marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n",
    "    The marker style. *marker* can be either an instance of the class\n",
    "    or the text shorthand for a particular marker.\n",
    "    See :mod:`matplotlib.markers` for more information about marker\n",
    "    styles.\n",
    "\n",
    "cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
    "    The Colormap instance or registered colormap name used to map scalar data\n",
    "    to colors.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "norm : str or `~matplotlib.colors.Normalize`, optional\n",
    "    The normalization method used to scale scalar data to the [0, 1] range\n",
    "    before mapping to colors using *cmap*. By default, a linear scaling is\n",
    "    used, mapping the lowest value to 0 and the highest to 1.\n",
    "\n",
    "    If given, this can be one of the following:\n",
    "\n",
    "    - An instance of `.Normalize` or one of its subclasses\n",
    "      (see :ref:`colormapnorms`).\n",
    "    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n",
    "      list of available scales, call `matplotlib.scale.get_scale_names()`.\n",
    "      In that case, a suitable `.Normalize` subclass is dynamically generated\n",
    "      and instantiated.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "vmin, vmax : float, optional\n",
    "    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n",
    "    the data range that the colormap covers. By default, the colormap covers\n",
    "    the complete value range of the supplied data. It is an error to use\n",
    "    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n",
    "    name together with *vmin*/*vmax* is acceptable).\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "alpha : float, default: None\n",
    "    The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "\n",
    "linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
    "    The linewidth of the marker edges. Note: The default *edgecolors*\n",
    "    is 'face'. You may want to change this as well.\n",
    "\n",
    "edgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n",
    "    The edge color of the marker. Possible values:\n",
    "\n",
    "    - 'face': The edge color will always be the same as the face color.\n",
    "    - 'none': No patch boundary will be drawn.\n",
    "    - A color or sequence of colors.\n",
    "\n",
    "    For non-filled markers, *edgecolors* is ignored. Instead, the color\n",
    "    is determined like with 'face', i.e. from *c*, *colors*, or\n",
    "    *facecolors*.\n",
    "\n",
    "plotnonfinite : bool, default: False\n",
    "    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n",
    "    or ``nan``). If ``True`` the points are drawn with the *bad*\n",
    "    colormap color (see `.Colormap.set_bad`).\n",
    "\n",
    "Returns\n",
    "-------\n",
    "`~matplotlib.collections.PathCollection`\n",
    "\n",
    "Other Parameters\n",
    "----------------\n",
    "data : indexable object, optional\n",
    "    If given, the following parameters also accept a string ``s``, which is\n",
    "    interpreted as ``data[s]`` (unless this raises an exception):\n",
    "\n",
    "    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n",
    "**kwargs : `~matplotlib.collections.Collection` properties\n",
    "\n",
    "See Also\n",
    "--------\n",
    "plot : To plot scatter plots when markers are identical in size and\n",
    "    color.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "* The `.plot` function will be faster for scatterplots where markers\n",
    "  don't vary in size or color.\n",
    "\n",
    "* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n",
    "  case all masks will be combined and only unmasked points will be\n",
    "  plotted.\n",
    "\n",
    "* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n",
    "  may be input as N-D arrays, but within scatter they will be\n",
    "  flattened. The exception is *c*, which will be flattened only if its\n",
    "  size matches the size of *x* and *y*.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<li><details><summary style='list-style: none;'><h3><span style='color:#42a5f5'>Data Preparation</span></h3></summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Data Profiling and Exploratory Data Analysis</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Data Cleaning Filtering</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Data Sub-sampling and Train-test Splitting</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<li><details><summary style='list-style: none;'><h3><span style='color:#42a5f5'>Feature Engineering</span></h3></summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Feature Transformation</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<ul><li><details><summary style='list-style: none; cursor: pointer;'><strong>Feature Selection</strong></summary>\n",
    "<ul>\n",
    "\n",
    "<li><details><summary style='list-style: none; cursor: pointer;'><u>View All \"Feature Selection\" Calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_selection._rfe.RFE</u> | <b>(See Args)</b> </summary> <ul><li><b>Args:</b> [] | <b>Kwargs:</b> {'n_features_to_select': 10, 'step': 1}</li></ul>\n",
    "<blockquote>\n",
    "<code>\n",
    "Feature ranking with recursive feature elimination.\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the\n",
    "coefficients of a linear model), the goal of recursive feature elimination\n",
    "(RFE) is to select features by recursively considering smaller and smaller\n",
    "sets of features. First, the estimator is trained on the initial set of\n",
    "features and the importance of each feature is obtained either through\n",
    "any specific attribute or callable.\n",
    "Then, the least important features are pruned from current set of features.\n",
    "That procedure is recursively repeated on the pruned set until the desired\n",
    "number of features to select is eventually reached.\n",
    "\n",
    "Read more in the :ref:`User Guide <rfe>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "estimator : ``Estimator`` instance\n",
    "    A supervised learning estimator with a ``fit`` method that provides\n",
    "    information about feature importance\n",
    "    (e.g. `coef_`, `feature_importances_`).\n",
    "\n",
    "n_features_to_select : int or float, default=None\n",
    "    The number of features to select. If `None`, half of the features are\n",
    "    selected. If integer, the parameter is the absolute number of features\n",
    "    to select. If float between 0 and 1, it is the fraction of features to\n",
    "    select.\n",
    "\n",
    "    .. versionchanged:: 0.24\n",
    "       Added float values for fractions.\n",
    "\n",
    "step : int or float, default=1\n",
    "    If greater than or equal to 1, then ``step`` corresponds to the\n",
    "    (integer) number of features to remove at each iteration.\n",
    "    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n",
    "    (rounded down) of features to remove at each iteration.\n",
    "\n",
    "verbose : int, default=0\n",
    "    Controls verbosity of output.\n",
    "\n",
    "importance_getter : str or callable, default='auto'\n",
    "    If 'auto', uses the feature importance either through a `coef_`\n",
    "    or `feature_importances_` attributes of estimator.\n",
    "\n",
    "    Also accepts a string that specifies an attribute name/path\n",
    "    for extracting feature importance (implemented with `attrgetter`).\n",
    "    For example, give `regressor_.coef_` in case of\n",
    "    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n",
    "    `named_steps.clf.feature_importances_` in case of\n",
    "    class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n",
    "\n",
    "    If `callable`, overrides the default feature importance getter.\n",
    "    The callable is passed with the fitted estimator and it should\n",
    "    return importance for each feature.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "classes_ : ndarray of shape (n_classes,)\n",
    "    The classes labels. Only available when `estimator` is a classifier.\n",
    "\n",
    "estimator_ : ``Estimator`` instance\n",
    "    The fitted estimator used to select features.\n",
    "\n",
    "n_features_ : int\n",
    "    The number of selected features.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`. Only defined if the\n",
    "    underlying estimator exposes such an attribute when fit.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "ranking_ : ndarray of shape (n_features,)\n",
    "    The feature ranking, such that ``ranking_[i]`` corresponds to the\n",
    "    ranking position of the i-th feature. Selected (i.e., estimated\n",
    "    best) features are assigned rank 1.\n",
    "\n",
    "support_ : ndarray of shape (n_features,)\n",
    "    The mask of selected features.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "RFECV : Recursive feature elimination with built-in cross-validated\n",
    "    selection of the best number of features.\n",
    "SelectFromModel : Feature selection based on thresholds of importance\n",
    "    weights.\n",
    "SequentialFeatureSelector : Sequential cross-validation based feature\n",
    "    selection. Does not rely on importance weights.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Allows NaN/Inf in the input if the underlying estimator does as well.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    ".. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n",
    "       for cancer classification using support vector machines\",\n",
    "       Mach. Learn., 46(1-3), 389--422, 2002.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "The following example shows how to retrieve the 5 most informative\n",
    "features in the Friedman #1 dataset.\n",
    "\n",
    ">>> from sklearn.datasets import make_friedman1\n",
    ">>> from sklearn.feature_selection import RFE\n",
    ">>> from sklearn.svm import SVR\n",
    ">>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    ">>> estimator = SVR(kernel=\"linear\")\n",
    ">>> selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    ">>> selector = selector.fit(X, y)\n",
    ">>> selector.support_\n",
    "array([ True,  True,  True,  True,  True, False, False, False, False,\n",
    "       False])\n",
    ">>> selector.ranking_\n",
    "array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 22</u></strong></summary><small><a href=#22>goto cell # 22</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_selection._rfe.RFE</u> | <b>(See Args)</b> </summary> <ul><li><b>Args:</b> [] | <b>Kwargs:</b> {'n_features_to_select': 10, 'step': 1}</li></ul>\n",
    "<blockquote>\n",
    "<code>\n",
    "Feature ranking with recursive feature elimination.\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the\n",
    "coefficients of a linear model), the goal of recursive feature elimination\n",
    "(RFE) is to select features by recursively considering smaller and smaller\n",
    "sets of features. First, the estimator is trained on the initial set of\n",
    "features and the importance of each feature is obtained either through\n",
    "any specific attribute or callable.\n",
    "Then, the least important features are pruned from current set of features.\n",
    "That procedure is recursively repeated on the pruned set until the desired\n",
    "number of features to select is eventually reached.\n",
    "\n",
    "Read more in the :ref:`User Guide <rfe>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "estimator : ``Estimator`` instance\n",
    "    A supervised learning estimator with a ``fit`` method that provides\n",
    "    information about feature importance\n",
    "    (e.g. `coef_`, `feature_importances_`).\n",
    "\n",
    "n_features_to_select : int or float, default=None\n",
    "    The number of features to select. If `None`, half of the features are\n",
    "    selected. If integer, the parameter is the absolute number of features\n",
    "    to select. If float between 0 and 1, it is the fraction of features to\n",
    "    select.\n",
    "\n",
    "    .. versionchanged:: 0.24\n",
    "       Added float values for fractions.\n",
    "\n",
    "step : int or float, default=1\n",
    "    If greater than or equal to 1, then ``step`` corresponds to the\n",
    "    (integer) number of features to remove at each iteration.\n",
    "    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n",
    "    (rounded down) of features to remove at each iteration.\n",
    "\n",
    "verbose : int, default=0\n",
    "    Controls verbosity of output.\n",
    "\n",
    "importance_getter : str or callable, default='auto'\n",
    "    If 'auto', uses the feature importance either through a `coef_`\n",
    "    or `feature_importances_` attributes of estimator.\n",
    "\n",
    "    Also accepts a string that specifies an attribute name/path\n",
    "    for extracting feature importance (implemented with `attrgetter`).\n",
    "    For example, give `regressor_.coef_` in case of\n",
    "    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n",
    "    `named_steps.clf.feature_importances_` in case of\n",
    "    class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n",
    "\n",
    "    If `callable`, overrides the default feature importance getter.\n",
    "    The callable is passed with the fitted estimator and it should\n",
    "    return importance for each feature.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "classes_ : ndarray of shape (n_classes,)\n",
    "    The classes labels. Only available when `estimator` is a classifier.\n",
    "\n",
    "estimator_ : ``Estimator`` instance\n",
    "    The fitted estimator used to select features.\n",
    "\n",
    "n_features_ : int\n",
    "    The number of selected features.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`. Only defined if the\n",
    "    underlying estimator exposes such an attribute when fit.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "ranking_ : ndarray of shape (n_features,)\n",
    "    The feature ranking, such that ``ranking_[i]`` corresponds to the\n",
    "    ranking position of the i-th feature. Selected (i.e., estimated\n",
    "    best) features are assigned rank 1.\n",
    "\n",
    "support_ : ndarray of shape (n_features,)\n",
    "    The mask of selected features.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "RFECV : Recursive feature elimination with built-in cross-validated\n",
    "    selection of the best number of features.\n",
    "SelectFromModel : Feature selection based on thresholds of importance\n",
    "    weights.\n",
    "SequentialFeatureSelector : Sequential cross-validation based feature\n",
    "    selection. Does not rely on importance weights.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Allows NaN/Inf in the input if the underlying estimator does as well.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    ".. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n",
    "       for cancer classification using support vector machines\",\n",
    "       Mach. Learn., 46(1-3), 389--422, 2002.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "The following example shows how to retrieve the 5 most informative\n",
    "features in the Friedman #1 dataset.\n",
    "\n",
    ">>> from sklearn.datasets import make_friedman1\n",
    ">>> from sklearn.feature_selection import RFE\n",
    ">>> from sklearn.svm import SVR\n",
    ">>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    ">>> estimator = SVR(kernel=\"linear\")\n",
    ">>> selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    ">>> selector = selector.fit(X, y)\n",
    ">>> selector.support_\n",
    "array([ True,  True,  True,  True,  True, False, False, False, False,\n",
    "       False])\n",
    ">>> selector.ranking_\n",
    "array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<li><details><summary style='list-style: none; cursor: pointer;'><h3><span style='color:#42a5f5'>Model Building and Training</span></h3></summary>\n",
    "<ul>\n",
    "\n",
    "<li><details><summary style='list-style: none; cursor: pointer;'><u>View All \"Model Building and Training\" Calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.ensemble._forest.RandomForestRegressor</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "A random forest regressor.\n",
    "\n",
    "A random forest is a meta estimator that fits a number of classifying\n",
    "decision trees on various sub-samples of the dataset and uses averaging\n",
    "to improve the predictive accuracy and control over-fitting.\n",
    "The sub-sample size is controlled with the `max_samples` parameter if\n",
    "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
    "each tree.\n",
    "\n",
    "Read more in the :ref:`User Guide <forest>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "n_estimators : int, default=100\n",
    "    The number of trees in the forest.\n",
    "\n",
    "    .. versionchanged:: 0.22\n",
    "       The default value of ``n_estimators`` changed from 10 to 100\n",
    "       in 0.22.\n",
    "\n",
    "criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"\n",
    "    The function to measure the quality of a split. Supported criteria\n",
    "    are \"squared_error\" for the mean squared error, which is equal to\n",
    "    variance reduction as feature selection criterion and minimizes the L2\n",
    "    loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
    "    mean squared error with Friedman's improvement score for potential\n",
    "    splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
    "    the L1 loss using the median of each terminal node, and \"poisson\" which\n",
    "    uses reduction in Poisson deviance to find splits.\n",
    "    Training using \"absolute_error\" is significantly slower\n",
    "    than when using \"squared_error\".\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Mean Absolute Error (MAE) criterion.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "       Poisson criterion.\n",
    "\n",
    "max_depth : int, default=None\n",
    "    The maximum depth of the tree. If None, then nodes are expanded until\n",
    "    all leaves are pure or until all leaves contain less than\n",
    "    min_samples_split samples.\n",
    "\n",
    "min_samples_split : int or float, default=2\n",
    "    The minimum number of samples required to split an internal node:\n",
    "\n",
    "    - If int, then consider `min_samples_split` as the minimum number.\n",
    "    - If float, then `min_samples_split` is a fraction and\n",
    "      `ceil(min_samples_split * n_samples)` are the minimum\n",
    "      number of samples for each split.\n",
    "\n",
    "    .. versionchanged:: 0.18\n",
    "       Added float values for fractions.\n",
    "\n",
    "min_samples_leaf : int or float, default=1\n",
    "    The minimum number of samples required to be at a leaf node.\n",
    "    A split point at any depth will only be considered if it leaves at\n",
    "    least ``min_samples_leaf`` training samples in each of the left and\n",
    "    right branches.  This may have the effect of smoothing the model,\n",
    "    especially in regression.\n",
    "\n",
    "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "    - If float, then `min_samples_leaf` is a fraction and\n",
    "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "      number of samples for each node.\n",
    "\n",
    "    .. versionchanged:: 0.18\n",
    "       Added float values for fractions.\n",
    "\n",
    "min_weight_fraction_leaf : float, default=0.0\n",
    "    The minimum weighted fraction of the sum total of weights (of all\n",
    "    the input samples) required to be at a leaf node. Samples have\n",
    "    equal weight when sample_weight is not provided.\n",
    "\n",
    "max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
    "    The number of features to consider when looking for the best split:\n",
    "\n",
    "    - If int, then consider `max_features` features at each split.\n",
    "    - If float, then `max_features` is a fraction and\n",
    "      `max(1, int(max_features * n_features_in_))` features are considered at each\n",
    "      split.\n",
    "    - If \"auto\", then `max_features=n_features`.\n",
    "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "    - If \"log2\", then `max_features=log2(n_features)`.\n",
    "    - If None or 1.0, then `max_features=n_features`.\n",
    "\n",
    "    .. note::\n",
    "        The default of 1.0 is equivalent to bagged trees and more\n",
    "        randomness can be achieved by setting smaller values, e.g. 0.3.\n",
    "\n",
    "    .. versionchanged:: 1.1\n",
    "        The default of `max_features` changed from `\"auto\"` to 1.0.\n",
    "\n",
    "    .. deprecated:: 1.1\n",
    "        The `\"auto\"` option was deprecated in 1.1 and will be removed\n",
    "        in 1.3.\n",
    "\n",
    "    Note: the search for a split does not stop until at least one\n",
    "    valid partition of the node samples is found, even if it requires to\n",
    "    effectively inspect more than ``max_features`` features.\n",
    "\n",
    "max_leaf_nodes : int, default=None\n",
    "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
    "    Best nodes are defined as relative reduction in impurity.\n",
    "    If None then unlimited number of leaf nodes.\n",
    "\n",
    "min_impurity_decrease : float, default=0.0\n",
    "    A node will be split if this split induces a decrease of the impurity\n",
    "    greater than or equal to this value.\n",
    "\n",
    "    The weighted impurity decrease equation is the following::\n",
    "\n",
    "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                            - N_t_L / N_t * left_impurity)\n",
    "\n",
    "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "\n",
    "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "    if ``sample_weight`` is passed.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "\n",
    "bootstrap : bool, default=True\n",
    "    Whether bootstrap samples are used when building trees. If False, the\n",
    "    whole dataset is used to build each tree.\n",
    "\n",
    "oob_score : bool, default=False\n",
    "    Whether to use out-of-bag samples to estimate the generalization score.\n",
    "    Only available if bootstrap=True.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
    "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
    "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors. See :term:`Glossary\n",
    "    <n_jobs>` for more details.\n",
    "\n",
    "random_state : int, RandomState instance or None, default=None\n",
    "    Controls both the randomness of the bootstrapping of the samples used\n",
    "    when building trees (if ``bootstrap=True``) and the sampling of the\n",
    "    features to consider when looking for the best split at each node\n",
    "    (if ``max_features < n_features``).\n",
    "    See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "verbose : int, default=0\n",
    "    Controls the verbosity when fitting and predicting.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to ``True``, reuse the solution of the previous call to fit\n",
    "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
    "    new forest. See :term:`Glossary <warm_start>` and\n",
    "    :ref:`gradient_boosting_warm_start` for details.\n",
    "\n",
    "ccp_alpha : non-negative float, default=0.0\n",
    "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
    "    subtree with the largest cost complexity that is smaller than\n",
    "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
    "    :ref:`minimal_cost_complexity_pruning` for details.\n",
    "\n",
    "    .. versionadded:: 0.22\n",
    "\n",
    "max_samples : int or float, default=None\n",
    "    If bootstrap is True, the number of samples to draw from X\n",
    "    to train each base estimator.\n",
    "\n",
    "    - If None (default), then draw `X.shape[0]` samples.\n",
    "    - If int, then draw `max_samples` samples.\n",
    "    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
    "      `max_samples` should be in the interval `(0.0, 1.0]`.\n",
    "\n",
    "    .. versionadded:: 0.22\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`\n",
    "    The child estimator template used to create the collection of fitted\n",
    "    sub-estimators.\n",
    "\n",
    "    .. versionadded:: 1.2\n",
    "       `base_estimator_` was renamed to `estimator_`.\n",
    "\n",
    "base_estimator_ : DecisionTreeRegressor\n",
    "    The child estimator template used to create the collection of fitted\n",
    "    sub-estimators.\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "        `base_estimator_` is deprecated and will be removed in 1.4.\n",
    "        Use `estimator_` instead.\n",
    "\n",
    "estimators_ : list of DecisionTreeRegressor\n",
    "    The collection of fitted sub-estimators.\n",
    "\n",
    "feature_importances_ : ndarray of shape (n_features,)\n",
    "    The impurity-based feature importances.\n",
    "    The higher, the more important the feature.\n",
    "    The importance of a feature is computed as the (normalized)\n",
    "    total reduction of the criterion brought by that feature.  It is also\n",
    "    known as the Gini importance.\n",
    "\n",
    "    Warning: impurity-based feature importances can be misleading for\n",
    "    high cardinality features (many unique values). See\n",
    "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_outputs_ : int\n",
    "    The number of outputs when ``fit`` is performed.\n",
    "\n",
    "oob_score_ : float\n",
    "    Score of the training dataset obtained using an out-of-bag estimate.\n",
    "    This attribute exists only when ``oob_score`` is True.\n",
    "\n",
    "oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Prediction computed with out-of-bag estimate on the training set.\n",
    "    This attribute exists only when ``oob_score`` is True.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
    "sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
    "    tree regressors.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The default values for the parameters controlling the size of the trees\n",
    "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "unpruned trees which can potentially be very large on some data sets. To\n",
    "reduce memory consumption, the complexity and size of the trees should be\n",
    "controlled by setting those parameter values.\n",
    "\n",
    "The features are always randomly permuted at each split. Therefore,\n",
    "the best found split may vary, even with the same training data,\n",
    "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
    "of the criterion is identical for several splits enumerated during the\n",
    "search of the best split. To obtain a deterministic behaviour during\n",
    "fitting, ``random_state`` has to be fixed.\n",
    "\n",
    "The default value ``max_features=\"auto\"`` uses ``n_features``\n",
    "rather than ``n_features / 3``. The latter was originally suggested in\n",
    "[1], whereas the former was more recently justified empirically in [2].\n",
    "\n",
    "References\n",
    "----------\n",
    ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
    "\n",
    ".. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
    "       trees\", Machine Learning, 63(1), 3-42, 2006.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.ensemble import RandomForestRegressor\n",
    ">>> from sklearn.datasets import make_regression\n",
    ">>> X, y = make_regression(n_features=4, n_informative=2,\n",
    "...                        random_state=0, shuffle=False)\n",
    ">>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    ">>> regr.fit(X, y)\n",
    "RandomForestRegressor(...)\n",
    ">>> print(regr.predict([[0, 0, 0, 0]]))\n",
    "[-8.32987858]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._base.LinearRegression</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Ordinary least squares Linear Regression.\n",
    "\n",
    "LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
    "to minimize the residual sum of squares between the observed targets in\n",
    "the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "fit_intercept : bool, default=True\n",
    "    Whether to calculate the intercept for this model. If set\n",
    "    to False, no intercept will be used in calculations\n",
    "    (i.e. data is expected to be centered).\n",
    "\n",
    "copy_X : bool, default=True\n",
    "    If True, X will be copied; else, it may be overwritten.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    The number of jobs to use for the computation. This will only provide\n",
    "    speedup in case of sufficiently large problems, that is if firstly\n",
    "    `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n",
    "    to `True`. ``None`` means 1 unless in a\n",
    "    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
    "    processors. See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "positive : bool, default=False\n",
    "    When set to ``True``, forces the coefficients to be positive. This\n",
    "    option is only supported for dense arrays.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
    "    Estimated coefficients for the linear regression problem.\n",
    "    If multiple targets are passed during the fit (y 2D), this\n",
    "    is a 2D array of shape (n_targets, n_features), while if only\n",
    "    one target is passed, this is a 1D array of length n_features.\n",
    "\n",
    "rank_ : int\n",
    "    Rank of matrix `X`. Only available when `X` is dense.\n",
    "\n",
    "singular_ : array of shape (min(X, y),)\n",
    "    Singular values of `X`. Only available when `X` is dense.\n",
    "\n",
    "intercept_ : float or array of shape (n_targets,)\n",
    "    Independent term in the linear model. Set to 0.0 if\n",
    "    `fit_intercept = False`.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Ridge : Ridge regression addresses some of the\n",
    "    problems of Ordinary Least Squares by imposing a penalty on the\n",
    "    size of the coefficients with l2 regularization.\n",
    "Lasso : The Lasso is a linear model that estimates\n",
    "    sparse coefficients with l1 regularization.\n",
    "ElasticNet : Elastic-Net is a linear regression\n",
    "    model trained with both l1 and l2 -norm regularization of the\n",
    "    coefficients.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "From the implementation point of view, this is just plain Ordinary\n",
    "Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n",
    "(scipy.optimize.nnls) wrapped as a predictor object.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> import numpy as np\n",
    ">>> from sklearn.linear_model import LinearRegression\n",
    ">>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    ">>> # y = 1 * x_0 + 2 * x_1 + 3\n",
    ">>> y = np.dot(X, np.array([1, 2])) + 3\n",
    ">>> reg = LinearRegression().fit(X, y)\n",
    ">>> reg.score(X, y)\n",
    "1.0\n",
    ">>> reg.coef_\n",
    "array([1., 2.])\n",
    ">>> reg.intercept_\n",
    "3.0...\n",
    ">>> reg.predict(np.array([[3, 5]]))\n",
    "array([16.])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 5</u></strong></summary><small><a href=#5>goto cell # 5</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.ensemble._forest.RandomForestRegressor</u> | <b>(See Args)</b> </summary> <ul><li><b>Args:</b> [] | <b>Kwargs:</b> {'n_estimators': 200}</li></ul>\n",
    "<blockquote>\n",
    "<code>\n",
    "A random forest regressor.\n",
    "\n",
    "A random forest is a meta estimator that fits a number of classifying\n",
    "decision trees on various sub-samples of the dataset and uses averaging\n",
    "to improve the predictive accuracy and control over-fitting.\n",
    "The sub-sample size is controlled with the `max_samples` parameter if\n",
    "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
    "each tree.\n",
    "\n",
    "Read more in the :ref:`User Guide <forest>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "n_estimators : int, default=100\n",
    "    The number of trees in the forest.\n",
    "\n",
    "    .. versionchanged:: 0.22\n",
    "       The default value of ``n_estimators`` changed from 10 to 100\n",
    "       in 0.22.\n",
    "\n",
    "criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"\n",
    "    The function to measure the quality of a split. Supported criteria\n",
    "    are \"squared_error\" for the mean squared error, which is equal to\n",
    "    variance reduction as feature selection criterion and minimizes the L2\n",
    "    loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
    "    mean squared error with Friedman's improvement score for potential\n",
    "    splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
    "    the L1 loss using the median of each terminal node, and \"poisson\" which\n",
    "    uses reduction in Poisson deviance to find splits.\n",
    "    Training using \"absolute_error\" is significantly slower\n",
    "    than when using \"squared_error\".\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Mean Absolute Error (MAE) criterion.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "       Poisson criterion.\n",
    "\n",
    "max_depth : int, default=None\n",
    "    The maximum depth of the tree. If None, then nodes are expanded until\n",
    "    all leaves are pure or until all leaves contain less than\n",
    "    min_samples_split samples.\n",
    "\n",
    "min_samples_split : int or float, default=2\n",
    "    The minimum number of samples required to split an internal node:\n",
    "\n",
    "    - If int, then consider `min_samples_split` as the minimum number.\n",
    "    - If float, then `min_samples_split` is a fraction and\n",
    "      `ceil(min_samples_split * n_samples)` are the minimum\n",
    "      number of samples for each split.\n",
    "\n",
    "    .. versionchanged:: 0.18\n",
    "       Added float values for fractions.\n",
    "\n",
    "min_samples_leaf : int or float, default=1\n",
    "    The minimum number of samples required to be at a leaf node.\n",
    "    A split point at any depth will only be considered if it leaves at\n",
    "    least ``min_samples_leaf`` training samples in each of the left and\n",
    "    right branches.  This may have the effect of smoothing the model,\n",
    "    especially in regression.\n",
    "\n",
    "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "    - If float, then `min_samples_leaf` is a fraction and\n",
    "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "      number of samples for each node.\n",
    "\n",
    "    .. versionchanged:: 0.18\n",
    "       Added float values for fractions.\n",
    "\n",
    "min_weight_fraction_leaf : float, default=0.0\n",
    "    The minimum weighted fraction of the sum total of weights (of all\n",
    "    the input samples) required to be at a leaf node. Samples have\n",
    "    equal weight when sample_weight is not provided.\n",
    "\n",
    "max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
    "    The number of features to consider when looking for the best split:\n",
    "\n",
    "    - If int, then consider `max_features` features at each split.\n",
    "    - If float, then `max_features` is a fraction and\n",
    "      `max(1, int(max_features * n_features_in_))` features are considered at each\n",
    "      split.\n",
    "    - If \"auto\", then `max_features=n_features`.\n",
    "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "    - If \"log2\", then `max_features=log2(n_features)`.\n",
    "    - If None or 1.0, then `max_features=n_features`.\n",
    "\n",
    "    .. note::\n",
    "        The default of 1.0 is equivalent to bagged trees and more\n",
    "        randomness can be achieved by setting smaller values, e.g. 0.3.\n",
    "\n",
    "    .. versionchanged:: 1.1\n",
    "        The default of `max_features` changed from `\"auto\"` to 1.0.\n",
    "\n",
    "    .. deprecated:: 1.1\n",
    "        The `\"auto\"` option was deprecated in 1.1 and will be removed\n",
    "        in 1.3.\n",
    "\n",
    "    Note: the search for a split does not stop until at least one\n",
    "    valid partition of the node samples is found, even if it requires to\n",
    "    effectively inspect more than ``max_features`` features.\n",
    "\n",
    "max_leaf_nodes : int, default=None\n",
    "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
    "    Best nodes are defined as relative reduction in impurity.\n",
    "    If None then unlimited number of leaf nodes.\n",
    "\n",
    "min_impurity_decrease : float, default=0.0\n",
    "    A node will be split if this split induces a decrease of the impurity\n",
    "    greater than or equal to this value.\n",
    "\n",
    "    The weighted impurity decrease equation is the following::\n",
    "\n",
    "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                            - N_t_L / N_t * left_impurity)\n",
    "\n",
    "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "\n",
    "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "    if ``sample_weight`` is passed.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "\n",
    "bootstrap : bool, default=True\n",
    "    Whether bootstrap samples are used when building trees. If False, the\n",
    "    whole dataset is used to build each tree.\n",
    "\n",
    "oob_score : bool, default=False\n",
    "    Whether to use out-of-bag samples to estimate the generalization score.\n",
    "    Only available if bootstrap=True.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
    "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
    "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors. See :term:`Glossary\n",
    "    <n_jobs>` for more details.\n",
    "\n",
    "random_state : int, RandomState instance or None, default=None\n",
    "    Controls both the randomness of the bootstrapping of the samples used\n",
    "    when building trees (if ``bootstrap=True``) and the sampling of the\n",
    "    features to consider when looking for the best split at each node\n",
    "    (if ``max_features < n_features``).\n",
    "    See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "verbose : int, default=0\n",
    "    Controls the verbosity when fitting and predicting.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to ``True``, reuse the solution of the previous call to fit\n",
    "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
    "    new forest. See :term:`Glossary <warm_start>` and\n",
    "    :ref:`gradient_boosting_warm_start` for details.\n",
    "\n",
    "ccp_alpha : non-negative float, default=0.0\n",
    "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
    "    subtree with the largest cost complexity that is smaller than\n",
    "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
    "    :ref:`minimal_cost_complexity_pruning` for details.\n",
    "\n",
    "    .. versionadded:: 0.22\n",
    "\n",
    "max_samples : int or float, default=None\n",
    "    If bootstrap is True, the number of samples to draw from X\n",
    "    to train each base estimator.\n",
    "\n",
    "    - If None (default), then draw `X.shape[0]` samples.\n",
    "    - If int, then draw `max_samples` samples.\n",
    "    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
    "      `max_samples` should be in the interval `(0.0, 1.0]`.\n",
    "\n",
    "    .. versionadded:: 0.22\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`\n",
    "    The child estimator template used to create the collection of fitted\n",
    "    sub-estimators.\n",
    "\n",
    "    .. versionadded:: 1.2\n",
    "       `base_estimator_` was renamed to `estimator_`.\n",
    "\n",
    "base_estimator_ : DecisionTreeRegressor\n",
    "    The child estimator template used to create the collection of fitted\n",
    "    sub-estimators.\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "        `base_estimator_` is deprecated and will be removed in 1.4.\n",
    "        Use `estimator_` instead.\n",
    "\n",
    "estimators_ : list of DecisionTreeRegressor\n",
    "    The collection of fitted sub-estimators.\n",
    "\n",
    "feature_importances_ : ndarray of shape (n_features,)\n",
    "    The impurity-based feature importances.\n",
    "    The higher, the more important the feature.\n",
    "    The importance of a feature is computed as the (normalized)\n",
    "    total reduction of the criterion brought by that feature.  It is also\n",
    "    known as the Gini importance.\n",
    "\n",
    "    Warning: impurity-based feature importances can be misleading for\n",
    "    high cardinality features (many unique values). See\n",
    "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_outputs_ : int\n",
    "    The number of outputs when ``fit`` is performed.\n",
    "\n",
    "oob_score_ : float\n",
    "    Score of the training dataset obtained using an out-of-bag estimate.\n",
    "    This attribute exists only when ``oob_score`` is True.\n",
    "\n",
    "oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Prediction computed with out-of-bag estimate on the training set.\n",
    "    This attribute exists only when ``oob_score`` is True.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
    "sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
    "    tree regressors.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The default values for the parameters controlling the size of the trees\n",
    "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "unpruned trees which can potentially be very large on some data sets. To\n",
    "reduce memory consumption, the complexity and size of the trees should be\n",
    "controlled by setting those parameter values.\n",
    "\n",
    "The features are always randomly permuted at each split. Therefore,\n",
    "the best found split may vary, even with the same training data,\n",
    "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
    "of the criterion is identical for several splits enumerated during the\n",
    "search of the best split. To obtain a deterministic behaviour during\n",
    "fitting, ``random_state`` has to be fixed.\n",
    "\n",
    "The default value ``max_features=\"auto\"`` uses ``n_features``\n",
    "rather than ``n_features / 3``. The latter was originally suggested in\n",
    "[1], whereas the former was more recently justified empirically in [2].\n",
    "\n",
    "References\n",
    "----------\n",
    ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
    "\n",
    ".. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
    "       trees\", Machine Learning, 63(1), 3-42, 2006.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.ensemble import RandomForestRegressor\n",
    ">>> from sklearn.datasets import make_regression\n",
    ">>> X, y = make_regression(n_features=4, n_informative=2,\n",
    "...                        random_state=0, shuffle=False)\n",
    ">>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    ">>> regr.fit(X, y)\n",
    "RandomForestRegressor(...)\n",
    ">>> print(regr.predict([[0, 0, 0, 0]]))\n",
    "[-8.32987858]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 12</u></strong></summary><small><a href=#12>goto cell # 12</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._base.LinearRegression</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Ordinary least squares Linear Regression.\n",
    "\n",
    "LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
    "to minimize the residual sum of squares between the observed targets in\n",
    "the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "fit_intercept : bool, default=True\n",
    "    Whether to calculate the intercept for this model. If set\n",
    "    to False, no intercept will be used in calculations\n",
    "    (i.e. data is expected to be centered).\n",
    "\n",
    "copy_X : bool, default=True\n",
    "    If True, X will be copied; else, it may be overwritten.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    The number of jobs to use for the computation. This will only provide\n",
    "    speedup in case of sufficiently large problems, that is if firstly\n",
    "    `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n",
    "    to `True`. ``None`` means 1 unless in a\n",
    "    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
    "    processors. See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "positive : bool, default=False\n",
    "    When set to ``True``, forces the coefficients to be positive. This\n",
    "    option is only supported for dense arrays.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
    "    Estimated coefficients for the linear regression problem.\n",
    "    If multiple targets are passed during the fit (y 2D), this\n",
    "    is a 2D array of shape (n_targets, n_features), while if only\n",
    "    one target is passed, this is a 1D array of length n_features.\n",
    "\n",
    "rank_ : int\n",
    "    Rank of matrix `X`. Only available when `X` is dense.\n",
    "\n",
    "singular_ : array of shape (min(X, y),)\n",
    "    Singular values of `X`. Only available when `X` is dense.\n",
    "\n",
    "intercept_ : float or array of shape (n_targets,)\n",
    "    Independent term in the linear model. Set to 0.0 if\n",
    "    `fit_intercept = False`.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Ridge : Ridge regression addresses some of the\n",
    "    problems of Ordinary Least Squares by imposing a penalty on the\n",
    "    size of the coefficients with l2 regularization.\n",
    "Lasso : The Lasso is a linear model that estimates\n",
    "    sparse coefficients with l1 regularization.\n",
    "ElasticNet : Elastic-Net is a linear regression\n",
    "    model trained with both l1 and l2 -norm regularization of the\n",
    "    coefficients.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "From the implementation point of view, this is just plain Ordinary\n",
    "Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n",
    "(scipy.optimize.nnls) wrapped as a predictor object.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> import numpy as np\n",
    ">>> from sklearn.linear_model import LinearRegression\n",
    ">>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    ">>> # y = 1 * x_0 + 2 * x_1 + 3\n",
    ">>> y = np.dot(X, np.array([1, 2])) + 3\n",
    ">>> reg = LinearRegression().fit(X, y)\n",
    ">>> reg.score(X, y)\n",
    "1.0\n",
    ">>> reg.coef_\n",
    "array([1., 2.])\n",
    ">>> reg.intercept_\n",
    "3.0...\n",
    ">>> reg.predict(np.array([[3, 5]]))\n",
    "array([16.])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<ul><li><details><summary style='list-style: none; cursor: pointer;'><strong>Model Training</strong></summary>\n",
    "<ul>\n",
    "\n",
    "<li><details><summary style='list-style: none; cursor: pointer;'><u>View All \"Model Training\" Calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.ensemble._forest.BaseForest.fit</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Build a forest of trees from the training set (X, y).\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    The training input samples. Internally, its dtype will be converted\n",
    "    to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "    converted into a sparse ``csc_matrix``.\n",
    "\n",
    "y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    The target values (class labels in classification, real numbers in\n",
    "    regression).\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights. If None, then samples are equally weighted. Splits\n",
    "    that would create child nodes with net zero or negative weight are\n",
    "    ignored while searching for a split in each node. In the case of\n",
    "    classification, splits are also ignored if they would result in any\n",
    "    single class carrying a negative weight in either child node.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "self : object\n",
    "    Fitted estimator.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._base.LinearRegression.fit</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Fit linear model.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    Training data.\n",
    "\n",
    "y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
    "    Target values. Will be cast to X's dtype if necessary.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Individual weights for each sample.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       parameter *sample_weight* support to LinearRegression.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "self : object\n",
    "    Fitted Estimator.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_selection._rfe.RFE.fit</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Fit the RFE model and then the underlying estimator on the selected features.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    The training input samples.\n",
    "\n",
    "y : array-like of shape (n_samples,)\n",
    "    The target values.\n",
    "\n",
    "**fit_params : dict\n",
    "    Additional parameters passed to the `fit` method of the underlying\n",
    "    estimator.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "self : object\n",
    "    Fitted estimator.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 5</u></strong></summary><small><a href=#5>goto cell # 5</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.ensemble._forest.BaseForest.fit</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Build a forest of trees from the training set (X, y).\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    The training input samples. Internally, its dtype will be converted\n",
    "    to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "    converted into a sparse ``csc_matrix``.\n",
    "\n",
    "y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    The target values (class labels in classification, real numbers in\n",
    "    regression).\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights. If None, then samples are equally weighted. Splits\n",
    "    that would create child nodes with net zero or negative weight are\n",
    "    ignored while searching for a split in each node. In the case of\n",
    "    classification, splits are also ignored if they would result in any\n",
    "    single class carrying a negative weight in either child node.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "self : object\n",
    "    Fitted estimator.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 12</u></strong></summary><small><a href=#12>goto cell # 12</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._base.LinearRegression.fit</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Fit linear model.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    Training data.\n",
    "\n",
    "y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
    "    Target values. Will be cast to X's dtype if necessary.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Individual weights for each sample.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       parameter *sample_weight* support to LinearRegression.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "self : object\n",
    "    Fitted Estimator.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 22</u></strong></summary><small><a href=#22>goto cell # 22</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_selection._rfe.RFE.fit</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Fit the RFE model and then the underlying estimator on the selected features.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    The training input samples.\n",
    "\n",
    "y : array-like of shape (n_samples,)\n",
    "    The target values.\n",
    "\n",
    "**fit_params : dict\n",
    "    Additional parameters passed to the `fit` method of the underlying\n",
    "    estimator.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "self : object\n",
    "    Fitted estimator.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Model Parameter Tuning</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<ul><li><details><summary style='list-style: none; cursor: pointer;'><strong>Model Validation and Assembling</strong></summary>\n",
    "<ul>\n",
    "\n",
    "<li><details><summary style='list-style: none; cursor: pointer;'><u>View All \"Model Validation and Assembling\" Calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.ensemble._forest.ForestRegressor.predict</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Predict regression target for X.\n",
    "\n",
    "The predicted regression target of an input sample is computed as the\n",
    "mean predicted regression targets of the trees in the forest.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    The input samples. Internally, its dtype will be converted to\n",
    "    ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "    converted into a sparse ``csr_matrix``.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    The predicted values.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.r2_score</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    ":math:`R^2` (coefficient of determination) regression score function.\n",
    "\n",
    "Best possible score is 1.0 and it can be negative (because the\n",
    "model can be arbitrarily worse). In the general case when the true y is\n",
    "non-constant, a constant model that always predicts the average y\n",
    "disregarding the input features would get a :math:`R^2` score of 0.0.\n",
    "\n",
    "In the particular case when ``y_true`` is constant, the :math:`R^2` score\n",
    "is not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``\n",
    "(imperfect predictions). To prevent such non-finite numbers to pollute\n",
    "higher-level experiments such as a grid search cross-validation, by default\n",
    "these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect\n",
    "predictions) respectively. You can set ``force_finite`` to ``False`` to\n",
    "prevent this fix from happening.\n",
    "\n",
    "Note: when the prediction residuals have zero mean, the :math:`R^2` score\n",
    "is identical to the\n",
    ":func:`Explained Variance score <explained_variance_score>`.\n",
    "\n",
    "Read more in the :ref:`User Guide <r2_score>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n",
    "\n",
    "    Defines aggregating of multiple output scores.\n",
    "    Array-like value defines weights used to average scores.\n",
    "    Default is \"uniform_average\".\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of scores in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Scores of all outputs are averaged with uniform weight.\n",
    "\n",
    "    'variance_weighted' :\n",
    "        Scores of all outputs are averaged, weighted by the variances\n",
    "        of each individual output.\n",
    "\n",
    "    .. versionchanged:: 0.19\n",
    "        Default value of multioutput is 'uniform_average'.\n",
    "\n",
    "force_finite : bool, default=True\n",
    "    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n",
    "    data should be replaced with real numbers (``1.0`` if prediction is\n",
    "    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n",
    "    for hyperparameters' search procedures (e.g. grid search\n",
    "    cross-validation).\n",
    "\n",
    "    .. versionadded:: 1.1\n",
    "\n",
    "Returns\n",
    "-------\n",
    "z : float or ndarray of floats\n",
    "    The :math:`R^2` score or ndarray of scores if 'multioutput' is\n",
    "    'raw_values'.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "This is not a symmetric function.\n",
    "\n",
    "Unlike most other scores, :math:`R^2` score may be negative (it need not\n",
    "actually be the square of a quantity R).\n",
    "\n",
    "This metric is not well-defined for single samples and will return a NaN\n",
    "value if n_samples is less than two.\n",
    "\n",
    "References\n",
    "----------\n",
    ".. [1] `Wikipedia entry on the Coefficient of determination\n",
    "        <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import r2_score\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.948...\n",
    ">>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    ">>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    ">>> r2_score(y_true, y_pred,\n",
    "...          multioutput='variance_weighted')\n",
    "0.938...\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [1, 2, 3]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "1.0\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [2, 2, 2]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.0\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [3, 2, 1]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "-3.0\n",
    ">>> y_true = [-2, -2, -2]\n",
    ">>> y_pred = [-2, -2, -2]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "1.0\n",
    ">>> r2_score(y_true, y_pred, force_finite=False)\n",
    "nan\n",
    ">>> y_true = [-2, -2, -2]\n",
    ">>> y_pred = [-2, -2, -2 + 1e-8]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.0\n",
    ">>> r2_score(y_true, y_pred, force_finite=False)\n",
    "-inf\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.mean_squared_error</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Mean squared error regression loss.\n",
    "\n",
    "Read more in the :ref:`User Guide <mean_squared_error>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
    "    Defines aggregating of multiple output values.\n",
    "    Array-like value defines weights used to average errors.\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of errors in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Errors of all outputs are averaged with uniform weight.\n",
    "\n",
    "squared : bool, default=True\n",
    "    If True returns MSE value, if False returns RMSE value.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "loss : float or ndarray of floats\n",
    "    A non-negative floating point value (the best value is 0.0), or an\n",
    "    array of floating point values, one for each individual target.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import mean_squared_error\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.375\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.612...\n",
    ">>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
    ">>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.708...\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.822...\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "array([0.41666667, 1.        ])\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
    "0.825...\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._base.LinearModel.predict</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Predict using the linear model.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "    Samples.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "C : array, shape (n_samples,)\n",
    "    Returns predicted values.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_selection._rfe.RFE.predict</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Reduce X to the selected features and predict using the estimator.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : array of shape [n_samples, n_features]\n",
    "    The input samples.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "y : array of shape [n_samples]\n",
    "    The predicted target values.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 6</u></strong></summary><small><a href=#6>goto cell # 6</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.ensemble._forest.ForestRegressor.predict</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Predict regression target for X.\n",
    "\n",
    "The predicted regression target of an input sample is computed as the\n",
    "mean predicted regression targets of the trees in the forest.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    The input samples. Internally, its dtype will be converted to\n",
    "    ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "    converted into a sparse ``csr_matrix``.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    The predicted values.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 8</u></strong></summary><small><a href=#8>goto cell # 8</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.r2_score</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    ":math:`R^2` (coefficient of determination) regression score function.\n",
    "\n",
    "Best possible score is 1.0 and it can be negative (because the\n",
    "model can be arbitrarily worse). In the general case when the true y is\n",
    "non-constant, a constant model that always predicts the average y\n",
    "disregarding the input features would get a :math:`R^2` score of 0.0.\n",
    "\n",
    "In the particular case when ``y_true`` is constant, the :math:`R^2` score\n",
    "is not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``\n",
    "(imperfect predictions). To prevent such non-finite numbers to pollute\n",
    "higher-level experiments such as a grid search cross-validation, by default\n",
    "these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect\n",
    "predictions) respectively. You can set ``force_finite`` to ``False`` to\n",
    "prevent this fix from happening.\n",
    "\n",
    "Note: when the prediction residuals have zero mean, the :math:`R^2` score\n",
    "is identical to the\n",
    ":func:`Explained Variance score <explained_variance_score>`.\n",
    "\n",
    "Read more in the :ref:`User Guide <r2_score>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n",
    "\n",
    "    Defines aggregating of multiple output scores.\n",
    "    Array-like value defines weights used to average scores.\n",
    "    Default is \"uniform_average\".\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of scores in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Scores of all outputs are averaged with uniform weight.\n",
    "\n",
    "    'variance_weighted' :\n",
    "        Scores of all outputs are averaged, weighted by the variances\n",
    "        of each individual output.\n",
    "\n",
    "    .. versionchanged:: 0.19\n",
    "        Default value of multioutput is 'uniform_average'.\n",
    "\n",
    "force_finite : bool, default=True\n",
    "    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n",
    "    data should be replaced with real numbers (``1.0`` if prediction is\n",
    "    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n",
    "    for hyperparameters' search procedures (e.g. grid search\n",
    "    cross-validation).\n",
    "\n",
    "    .. versionadded:: 1.1\n",
    "\n",
    "Returns\n",
    "-------\n",
    "z : float or ndarray of floats\n",
    "    The :math:`R^2` score or ndarray of scores if 'multioutput' is\n",
    "    'raw_values'.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "This is not a symmetric function.\n",
    "\n",
    "Unlike most other scores, :math:`R^2` score may be negative (it need not\n",
    "actually be the square of a quantity R).\n",
    "\n",
    "This metric is not well-defined for single samples and will return a NaN\n",
    "value if n_samples is less than two.\n",
    "\n",
    "References\n",
    "----------\n",
    ".. [1] `Wikipedia entry on the Coefficient of determination\n",
    "        <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import r2_score\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.948...\n",
    ">>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    ">>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    ">>> r2_score(y_true, y_pred,\n",
    "...          multioutput='variance_weighted')\n",
    "0.938...\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [1, 2, 3]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "1.0\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [2, 2, 2]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.0\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [3, 2, 1]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "-3.0\n",
    ">>> y_true = [-2, -2, -2]\n",
    ">>> y_pred = [-2, -2, -2]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "1.0\n",
    ">>> r2_score(y_true, y_pred, force_finite=False)\n",
    "nan\n",
    ">>> y_true = [-2, -2, -2]\n",
    ">>> y_pred = [-2, -2, -2 + 1e-8]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.0\n",
    ">>> r2_score(y_true, y_pred, force_finite=False)\n",
    "-inf\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 9</u></strong></summary><small><a href=#9>goto cell # 9</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.mean_squared_error</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Mean squared error regression loss.\n",
    "\n",
    "Read more in the :ref:`User Guide <mean_squared_error>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
    "    Defines aggregating of multiple output values.\n",
    "    Array-like value defines weights used to average errors.\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of errors in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Errors of all outputs are averaged with uniform weight.\n",
    "\n",
    "squared : bool, default=True\n",
    "    If True returns MSE value, if False returns RMSE value.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "loss : float or ndarray of floats\n",
    "    A non-negative floating point value (the best value is 0.0), or an\n",
    "    array of floating point values, one for each individual target.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import mean_squared_error\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.375\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.612...\n",
    ">>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
    ">>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.708...\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.822...\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "array([0.41666667, 1.        ])\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
    "0.825...\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 12</u></strong></summary><small><a href=#12>goto cell # 12</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._base.LinearModel.predict</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Predict using the linear model.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "    Samples.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "C : array, shape (n_samples,)\n",
    "    Returns predicted values.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 13</u></strong></summary><small><a href=#13>goto cell # 13</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.r2_score</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    ":math:`R^2` (coefficient of determination) regression score function.\n",
    "\n",
    "Best possible score is 1.0 and it can be negative (because the\n",
    "model can be arbitrarily worse). In the general case when the true y is\n",
    "non-constant, a constant model that always predicts the average y\n",
    "disregarding the input features would get a :math:`R^2` score of 0.0.\n",
    "\n",
    "In the particular case when ``y_true`` is constant, the :math:`R^2` score\n",
    "is not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``\n",
    "(imperfect predictions). To prevent such non-finite numbers to pollute\n",
    "higher-level experiments such as a grid search cross-validation, by default\n",
    "these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect\n",
    "predictions) respectively. You can set ``force_finite`` to ``False`` to\n",
    "prevent this fix from happening.\n",
    "\n",
    "Note: when the prediction residuals have zero mean, the :math:`R^2` score\n",
    "is identical to the\n",
    ":func:`Explained Variance score <explained_variance_score>`.\n",
    "\n",
    "Read more in the :ref:`User Guide <r2_score>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n",
    "\n",
    "    Defines aggregating of multiple output scores.\n",
    "    Array-like value defines weights used to average scores.\n",
    "    Default is \"uniform_average\".\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of scores in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Scores of all outputs are averaged with uniform weight.\n",
    "\n",
    "    'variance_weighted' :\n",
    "        Scores of all outputs are averaged, weighted by the variances\n",
    "        of each individual output.\n",
    "\n",
    "    .. versionchanged:: 0.19\n",
    "        Default value of multioutput is 'uniform_average'.\n",
    "\n",
    "force_finite : bool, default=True\n",
    "    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n",
    "    data should be replaced with real numbers (``1.0`` if prediction is\n",
    "    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n",
    "    for hyperparameters' search procedures (e.g. grid search\n",
    "    cross-validation).\n",
    "\n",
    "    .. versionadded:: 1.1\n",
    "\n",
    "Returns\n",
    "-------\n",
    "z : float or ndarray of floats\n",
    "    The :math:`R^2` score or ndarray of scores if 'multioutput' is\n",
    "    'raw_values'.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "This is not a symmetric function.\n",
    "\n",
    "Unlike most other scores, :math:`R^2` score may be negative (it need not\n",
    "actually be the square of a quantity R).\n",
    "\n",
    "This metric is not well-defined for single samples and will return a NaN\n",
    "value if n_samples is less than two.\n",
    "\n",
    "References\n",
    "----------\n",
    ".. [1] `Wikipedia entry on the Coefficient of determination\n",
    "        <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import r2_score\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.948...\n",
    ">>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    ">>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    ">>> r2_score(y_true, y_pred,\n",
    "...          multioutput='variance_weighted')\n",
    "0.938...\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [1, 2, 3]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "1.0\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [2, 2, 2]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.0\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [3, 2, 1]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "-3.0\n",
    ">>> y_true = [-2, -2, -2]\n",
    ">>> y_pred = [-2, -2, -2]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "1.0\n",
    ">>> r2_score(y_true, y_pred, force_finite=False)\n",
    "nan\n",
    ">>> y_true = [-2, -2, -2]\n",
    ">>> y_pred = [-2, -2, -2 + 1e-8]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.0\n",
    ">>> r2_score(y_true, y_pred, force_finite=False)\n",
    "-inf\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 14</u></strong></summary><small><a href=#14>goto cell # 14</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.mean_squared_error</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Mean squared error regression loss.\n",
    "\n",
    "Read more in the :ref:`User Guide <mean_squared_error>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
    "    Defines aggregating of multiple output values.\n",
    "    Array-like value defines weights used to average errors.\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of errors in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Errors of all outputs are averaged with uniform weight.\n",
    "\n",
    "squared : bool, default=True\n",
    "    If True returns MSE value, if False returns RMSE value.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "loss : float or ndarray of floats\n",
    "    A non-negative floating point value (the best value is 0.0), or an\n",
    "    array of floating point values, one for each individual target.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import mean_squared_error\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.375\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.612...\n",
    ">>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
    ">>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.708...\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.822...\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "array([0.41666667, 1.        ])\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
    "0.825...\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 23</u></strong></summary><small><a href=#23>goto cell # 23</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_selection._rfe.RFE.predict</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Reduce X to the selected features and predict using the estimator.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : array of shape [n_samples, n_features]\n",
    "    The input samples.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "y : array of shape [n_samples]\n",
    "    The predicted target values.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 24</u></strong></summary><small><a href=#24>goto cell # 24</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.mean_squared_error</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Mean squared error regression loss.\n",
    "\n",
    "Read more in the :ref:`User Guide <mean_squared_error>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
    "    Defines aggregating of multiple output values.\n",
    "    Array-like value defines weights used to average errors.\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of errors in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Errors of all outputs are averaged with uniform weight.\n",
    "\n",
    "squared : bool, default=True\n",
    "    If True returns MSE value, if False returns RMSE value.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "loss : float or ndarray of floats\n",
    "    A non-negative floating point value (the best value is 0.0), or an\n",
    "    array of floating point values, one for each individual target.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import mean_squared_error\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.375\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.612...\n",
    ">>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
    ">>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.708...\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.822...\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "array([0.41666667, 1.        ])\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
    "0.825...\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "</ul>\n",
    "<hr>\n",
    "\n",
    "<details><summary style='list-style: none; cursor: pointer;'><strong>View All ML API Calls in Notebook</strong></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <b>builtins</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>builtins.list</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Built-in mutable sequence.\n",
    "\n",
    "If no argument is given, the constructor creates a new empty list.\n",
    "The argument must be an iterable if specified.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li> <b>matplotlib</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "`matplotlib.pyplot` is a state-based interface to matplotlib. It provides\n",
    "an implicit,  MATLAB-like, way of plotting.  It also opens figures on your\n",
    "screen, and acts as the figure GUI manager.\n",
    "\n",
    "pyplot is mainly intended for interactive plots and simple cases of\n",
    "programmatic plot generation::\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    x = np.arange(0, 5, 0.1)\n",
    "    y = np.sin(x)\n",
    "    plt.plot(x, y)\n",
    "\n",
    "The explicit object-oriented API is recommended for complex plots, though\n",
    "pyplot is still usually used to create the figure and often the axes in the\n",
    "figure. See `.pyplot.figure`, `.pyplot.subplots`, and\n",
    "`.pyplot.subplot_mosaic` to create figures, and\n",
    ":doc:`Axes API </api/axes_api>` for the plotting methods on an Axes::\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    x = np.arange(0, 5, 0.1)\n",
    "    y = np.sin(x)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "\n",
    "\n",
    "See :ref:`api_interfaces` for an explanation of the tradeoffs between the\n",
    "implicit and explicit interfaces.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot.plot</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Plot y versus x as lines and/or markers.\n",
    "\n",
    "Call signatures::\n",
    "\n",
    "    plot([x], y, [fmt], *, data=None, **kwargs)\n",
    "    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
    "\n",
    "The coordinates of the points or line nodes are given by *x*, *y*.\n",
    "\n",
    "The optional parameter *fmt* is a convenient way for defining basic\n",
    "formatting like color, marker and linestyle. It's a shortcut string\n",
    "notation described in the *Notes* section below.\n",
    "\n",
    ">>> plot(x, y)        # plot x and y using default line style and color\n",
    ">>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n",
    ">>> plot(y)           # plot y using x as index array 0..N-1\n",
    ">>> plot(y, 'r+')     # ditto, but with red plusses\n",
    "\n",
    "You can use `.Line2D` properties as keyword arguments for more\n",
    "control on the appearance. Line properties and *fmt* can be mixed.\n",
    "The following two calls yield identical results:\n",
    "\n",
    ">>> plot(x, y, 'go--', linewidth=2, markersize=12)\n",
    ">>> plot(x, y, color='green', marker='o', linestyle='dashed',\n",
    "...      linewidth=2, markersize=12)\n",
    "\n",
    "When conflicting with *fmt*, keyword arguments take precedence.\n",
    "\n",
    "\n",
    "**Plotting labelled data**\n",
    "\n",
    "There's a convenient way for plotting objects with labelled data (i.e.\n",
    "data that can be accessed by index ``obj['y']``). Instead of giving\n",
    "the data in *x* and *y*, you can provide the object in the *data*\n",
    "parameter and just give the labels for *x* and *y*::\n",
    "\n",
    ">>> plot('xlabel', 'ylabel', data=obj)\n",
    "\n",
    "All indexable objects are supported. This could e.g. be a `dict`, a\n",
    "`pandas.DataFrame` or a structured numpy array.\n",
    "\n",
    "\n",
    "**Plotting multiple sets of data**\n",
    "\n",
    "There are various ways to plot multiple sets of data.\n",
    "\n",
    "- The most straight forward way is just to call `plot` multiple times.\n",
    "  Example:\n",
    "\n",
    "  >>> plot(x1, y1, 'bo')\n",
    "  >>> plot(x2, y2, 'go')\n",
    "\n",
    "- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n",
    "  for every column. If both *x* and *y* are 2D, they must have the\n",
    "  same shape. If only one of them is 2D with shape (N, m) the other\n",
    "  must have length N and will be used for every data set m.\n",
    "\n",
    "  Example:\n",
    "\n",
    "  >>> x = [1, 2, 3]\n",
    "  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "  >>> plot(x, y)\n",
    "\n",
    "  is equivalent to:\n",
    "\n",
    "  >>> for col in range(y.shape[1]):\n",
    "  ...     plot(x, y[:, col])\n",
    "\n",
    "- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n",
    "  groups::\n",
    "\n",
    "  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n",
    "\n",
    "  In this case, any additional keyword argument applies to all\n",
    "  datasets. Also, this syntax cannot be combined with the *data*\n",
    "  parameter.\n",
    "\n",
    "By default, each line is assigned a different style specified by a\n",
    "'style cycle'. The *fmt* and line property parameters are only\n",
    "necessary if you want explicit deviations from these defaults.\n",
    "Alternatively, you can also change the style cycle using\n",
    ":rc:`axes.prop_cycle`.\n",
    "\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x, y : array-like or scalar\n",
    "    The horizontal / vertical coordinates of the data points.\n",
    "    *x* values are optional and default to ``range(len(y))``.\n",
    "\n",
    "    Commonly, these parameters are 1D arrays.\n",
    "\n",
    "    They can also be scalars, or two-dimensional (in that case, the\n",
    "    columns represent separate data sets).\n",
    "\n",
    "    These arguments cannot be passed as keywords.\n",
    "\n",
    "fmt : str, optional\n",
    "    A format string, e.g. 'ro' for red circles. See the *Notes*\n",
    "    section for a full description of the format strings.\n",
    "\n",
    "    Format strings are just an abbreviation for quickly setting\n",
    "    basic line properties. All of these and more can also be\n",
    "    controlled by keyword arguments.\n",
    "\n",
    "    This argument cannot be passed as keyword.\n",
    "\n",
    "data : indexable object, optional\n",
    "    An object with labelled data. If given, provide the label names to\n",
    "    plot in *x* and *y*.\n",
    "\n",
    "    .. note::\n",
    "        Technically there's a slight ambiguity in calls where the\n",
    "        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n",
    "        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n",
    "        the former interpretation is chosen, but a warning is issued.\n",
    "        You may suppress the warning by adding an empty format string\n",
    "        ``plot('n', 'o', '', data=obj)``.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "list of `.Line2D`\n",
    "    A list of lines representing the plotted data.\n",
    "\n",
    "Other Parameters\n",
    "----------------\n",
    "scalex, scaley : bool, default: True\n",
    "    These parameters determine if the view limits are adapted to the\n",
    "    data limits. The values are passed on to\n",
    "    `~.axes.Axes.autoscale_view`.\n",
    "\n",
    "**kwargs : `~matplotlib.lines.Line2D` properties, optional\n",
    "    *kwargs* are used to specify properties like a line label (for\n",
    "    auto legends), linewidth, antialiasing, marker face color.\n",
    "    Example::\n",
    "\n",
    "    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n",
    "    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n",
    "\n",
    "    If you specify multiple lines with one plot call, the kwargs apply\n",
    "    to all those lines. In case the label object is iterable, each\n",
    "    element is used as labels for each set of data.\n",
    "\n",
    "    Here is a list of available `.Line2D` properties:\n",
    "\n",
    "    Properties:\n",
    "    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n",
    "    alpha: scalar or None\n",
    "    animated: bool\n",
    "    antialiased or aa: bool\n",
    "    clip_box: `~matplotlib.transforms.BboxBase` or None\n",
    "    clip_on: bool\n",
    "    clip_path: Patch or (Path, Transform) or None\n",
    "    color or c: color\n",
    "    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n",
    "    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n",
    "    dashes: sequence of floats (on/off ink in points) or (None, None)\n",
    "    data: (2, N) array or two 1D arrays\n",
    "    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
    "    figure: `~matplotlib.figure.Figure`\n",
    "    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
    "    gapcolor: color or None\n",
    "    gid: str\n",
    "    in_layout: bool\n",
    "    label: object\n",
    "    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
    "    linewidth or lw: float\n",
    "    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
    "    markeredgecolor or mec: color\n",
    "    markeredgewidth or mew: float\n",
    "    markerfacecolor or mfc: color\n",
    "    markerfacecoloralt or mfcalt: color\n",
    "    markersize or ms: float\n",
    "    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n",
    "    mouseover: bool\n",
    "    path_effects: list of `.AbstractPathEffect`\n",
    "    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n",
    "    pickradius: float\n",
    "    rasterized: bool\n",
    "    sketch_params: (scale: float, length: float, randomness: float)\n",
    "    snap: bool or None\n",
    "    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n",
    "    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n",
    "    transform: unknown\n",
    "    url: str\n",
    "    visible: bool\n",
    "    xdata: 1D array\n",
    "    ydata: 1D array\n",
    "    zorder: float\n",
    "\n",
    "See Also\n",
    "--------\n",
    "scatter : XY scatter plot with markers of varying size and/or color (\n",
    "    sometimes also called bubble chart).\n",
    "\n",
    "Notes\n",
    "-----\n",
    "**Format Strings**\n",
    "\n",
    "A format string consists of a part for color, marker and line::\n",
    "\n",
    "    fmt = '[marker][line][color]'\n",
    "\n",
    "Each of them is optional. If not provided, the value from the style\n",
    "cycle is used. Exception: If ``line`` is given, but no ``marker``,\n",
    "the data will be a line without markers.\n",
    "\n",
    "Other combinations such as ``[color][marker][line]`` are also\n",
    "supported, but note that their parsing may be ambiguous.\n",
    "\n",
    "**Markers**\n",
    "\n",
    "=============   ===============================\n",
    "character       description\n",
    "=============   ===============================\n",
    "``'.'``         point marker\n",
    "``','``         pixel marker\n",
    "``'o'``         circle marker\n",
    "``'v'``         triangle_down marker\n",
    "``'^'``         triangle_up marker\n",
    "``'<'``         triangle_left marker\n",
    "``'>'``         triangle_right marker\n",
    "``'1'``         tri_down marker\n",
    "``'2'``         tri_up marker\n",
    "``'3'``         tri_left marker\n",
    "``'4'``         tri_right marker\n",
    "``'8'``         octagon marker\n",
    "``'s'``         square marker\n",
    "``'p'``         pentagon marker\n",
    "``'P'``         plus (filled) marker\n",
    "``'*'``         star marker\n",
    "``'h'``         hexagon1 marker\n",
    "``'H'``         hexagon2 marker\n",
    "``'+'``         plus marker\n",
    "``'x'``         x marker\n",
    "``'X'``         x (filled) marker\n",
    "``'D'``         diamond marker\n",
    "``'d'``         thin_diamond marker\n",
    "``'|'``         vline marker\n",
    "``'_'``         hline marker\n",
    "=============   ===============================\n",
    "\n",
    "**Line Styles**\n",
    "\n",
    "=============    ===============================\n",
    "character        description\n",
    "=============    ===============================\n",
    "``'-'``          solid line style\n",
    "``'--'``         dashed line style\n",
    "``'-.'``         dash-dot line style\n",
    "``':'``          dotted line style\n",
    "=============    ===============================\n",
    "\n",
    "Example format strings::\n",
    "\n",
    "    'b'    # blue markers with default shape\n",
    "    'or'   # red circles\n",
    "    '-g'   # green solid line\n",
    "    '--'   # dashed line with default color\n",
    "    '^k:'  # black triangle_up markers connected by a dotted line\n",
    "\n",
    "**Colors**\n",
    "\n",
    "The supported color abbreviations are the single letter codes\n",
    "\n",
    "=============    ===============================\n",
    "character        color\n",
    "=============    ===============================\n",
    "``'b'``          blue\n",
    "``'g'``          green\n",
    "``'r'``          red\n",
    "``'c'``          cyan\n",
    "``'m'``          magenta\n",
    "``'y'``          yellow\n",
    "``'k'``          black\n",
    "``'w'``          white\n",
    "=============    ===============================\n",
    "\n",
    "and the ``'CN'`` colors that index into the default property cycle.\n",
    "\n",
    "If the color is the only part of the format string, you can\n",
    "additionally use any  `matplotlib.colors` spec, e.g. full names\n",
    "(``'green'``) or hex strings (``'#008000'``).\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot.scatter</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x, y : float or array-like, shape (n, )\n",
    "    The data positions.\n",
    "\n",
    "s : float or array-like, shape (n, ), optional\n",
    "    The marker size in points**2 (typographic points are 1/72 in.).\n",
    "    Default is ``rcParams['lines.markersize'] ** 2``.\n",
    "\n",
    "    The linewidth and edgecolor can visually interact with the marker\n",
    "    size, and can lead to artifacts if the marker size is smaller than\n",
    "    the linewidth.\n",
    "\n",
    "    If the linewidth is greater than 0 and the edgecolor is anything\n",
    "    but *'none'*, then the effective size of the marker will be\n",
    "    increased by half the linewidth because the stroke will be centered\n",
    "    on the edge of the shape.\n",
    "\n",
    "    To eliminate the marker edge either set *linewidth=0* or\n",
    "    *edgecolor='none'*.\n",
    "\n",
    "c : array-like or list of colors or color, optional\n",
    "    The marker colors. Possible values:\n",
    "\n",
    "    - A scalar or sequence of n numbers to be mapped to colors using\n",
    "      *cmap* and *norm*.\n",
    "    - A 2D array in which the rows are RGB or RGBA.\n",
    "    - A sequence of colors of length n.\n",
    "    - A single color format string.\n",
    "\n",
    "    Note that *c* should not be a single numeric RGB or RGBA sequence\n",
    "    because that is indistinguishable from an array of values to be\n",
    "    colormapped. If you want to specify the same RGB or RGBA value for\n",
    "    all points, use a 2D array with a single row.  Otherwise,\n",
    "    value-matching will have precedence in case of a size matching with\n",
    "    *x* and *y*.\n",
    "\n",
    "    If you wish to specify a single color for all points\n",
    "    prefer the *color* keyword argument.\n",
    "\n",
    "    Defaults to `None`. In that case the marker color is determined\n",
    "    by the value of *color*, *facecolor* or *facecolors*. In case\n",
    "    those are not specified or `None`, the marker color is determined\n",
    "    by the next color of the ``Axes``' current \"shape and fill\" color\n",
    "    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n",
    "\n",
    "marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n",
    "    The marker style. *marker* can be either an instance of the class\n",
    "    or the text shorthand for a particular marker.\n",
    "    See :mod:`matplotlib.markers` for more information about marker\n",
    "    styles.\n",
    "\n",
    "cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
    "    The Colormap instance or registered colormap name used to map scalar data\n",
    "    to colors.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "norm : str or `~matplotlib.colors.Normalize`, optional\n",
    "    The normalization method used to scale scalar data to the [0, 1] range\n",
    "    before mapping to colors using *cmap*. By default, a linear scaling is\n",
    "    used, mapping the lowest value to 0 and the highest to 1.\n",
    "\n",
    "    If given, this can be one of the following:\n",
    "\n",
    "    - An instance of `.Normalize` or one of its subclasses\n",
    "      (see :ref:`colormapnorms`).\n",
    "    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n",
    "      list of available scales, call `matplotlib.scale.get_scale_names()`.\n",
    "      In that case, a suitable `.Normalize` subclass is dynamically generated\n",
    "      and instantiated.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "vmin, vmax : float, optional\n",
    "    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n",
    "    the data range that the colormap covers. By default, the colormap covers\n",
    "    the complete value range of the supplied data. It is an error to use\n",
    "    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n",
    "    name together with *vmin*/*vmax* is acceptable).\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "alpha : float, default: None\n",
    "    The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "\n",
    "linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
    "    The linewidth of the marker edges. Note: The default *edgecolors*\n",
    "    is 'face'. You may want to change this as well.\n",
    "\n",
    "edgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n",
    "    The edge color of the marker. Possible values:\n",
    "\n",
    "    - 'face': The edge color will always be the same as the face color.\n",
    "    - 'none': No patch boundary will be drawn.\n",
    "    - A color or sequence of colors.\n",
    "\n",
    "    For non-filled markers, *edgecolors* is ignored. Instead, the color\n",
    "    is determined like with 'face', i.e. from *c*, *colors*, or\n",
    "    *facecolors*.\n",
    "\n",
    "plotnonfinite : bool, default: False\n",
    "    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n",
    "    or ``nan``). If ``True`` the points are drawn with the *bad*\n",
    "    colormap color (see `.Colormap.set_bad`).\n",
    "\n",
    "Returns\n",
    "-------\n",
    "`~matplotlib.collections.PathCollection`\n",
    "\n",
    "Other Parameters\n",
    "----------------\n",
    "data : indexable object, optional\n",
    "    If given, the following parameters also accept a string ``s``, which is\n",
    "    interpreted as ``data[s]`` (unless this raises an exception):\n",
    "\n",
    "    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n",
    "**kwargs : `~matplotlib.collections.Collection` properties\n",
    "\n",
    "See Also\n",
    "--------\n",
    "plot : To plot scatter plots when markers are identical in size and\n",
    "    color.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "* The `.plot` function will be faster for scatterplots where markers\n",
    "  don't vary in size or color.\n",
    "\n",
    "* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n",
    "  case all masks will be combined and only unmasked points will be\n",
    "  plotted.\n",
    "\n",
    "* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n",
    "  may be input as N-D arrays, but within scatter they will be\n",
    "  flattened. The exception is *c*, which will be flattened only if its\n",
    "  size matches the size of *x* and *y*.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.ensemble._forest.BaseForest.fit</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Build a forest of trees from the training set (X, y).\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    The training input samples. Internally, its dtype will be converted\n",
    "    to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "    converted into a sparse ``csc_matrix``.\n",
    "\n",
    "y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    The target values (class labels in classification, real numbers in\n",
    "    regression).\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights. If None, then samples are equally weighted. Splits\n",
    "    that would create child nodes with net zero or negative weight are\n",
    "    ignored while searching for a split in each node. In the case of\n",
    "    classification, splits are also ignored if they would result in any\n",
    "    single class carrying a negative weight in either child node.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "self : object\n",
    "    Fitted estimator.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.ensemble._forest.ForestRegressor.predict</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Predict regression target for X.\n",
    "\n",
    "The predicted regression target of an input sample is computed as the\n",
    "mean predicted regression targets of the trees in the forest.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    The input samples. Internally, its dtype will be converted to\n",
    "    ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "    converted into a sparse ``csr_matrix``.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    The predicted values.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.ensemble._forest.RandomForestRegressor</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "A random forest regressor.\n",
    "\n",
    "A random forest is a meta estimator that fits a number of classifying\n",
    "decision trees on various sub-samples of the dataset and uses averaging\n",
    "to improve the predictive accuracy and control over-fitting.\n",
    "The sub-sample size is controlled with the `max_samples` parameter if\n",
    "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
    "each tree.\n",
    "\n",
    "Read more in the :ref:`User Guide <forest>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "n_estimators : int, default=100\n",
    "    The number of trees in the forest.\n",
    "\n",
    "    .. versionchanged:: 0.22\n",
    "       The default value of ``n_estimators`` changed from 10 to 100\n",
    "       in 0.22.\n",
    "\n",
    "criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"\n",
    "    The function to measure the quality of a split. Supported criteria\n",
    "    are \"squared_error\" for the mean squared error, which is equal to\n",
    "    variance reduction as feature selection criterion and minimizes the L2\n",
    "    loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
    "    mean squared error with Friedman's improvement score for potential\n",
    "    splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
    "    the L1 loss using the median of each terminal node, and \"poisson\" which\n",
    "    uses reduction in Poisson deviance to find splits.\n",
    "    Training using \"absolute_error\" is significantly slower\n",
    "    than when using \"squared_error\".\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Mean Absolute Error (MAE) criterion.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "       Poisson criterion.\n",
    "\n",
    "max_depth : int, default=None\n",
    "    The maximum depth of the tree. If None, then nodes are expanded until\n",
    "    all leaves are pure or until all leaves contain less than\n",
    "    min_samples_split samples.\n",
    "\n",
    "min_samples_split : int or float, default=2\n",
    "    The minimum number of samples required to split an internal node:\n",
    "\n",
    "    - If int, then consider `min_samples_split` as the minimum number.\n",
    "    - If float, then `min_samples_split` is a fraction and\n",
    "      `ceil(min_samples_split * n_samples)` are the minimum\n",
    "      number of samples for each split.\n",
    "\n",
    "    .. versionchanged:: 0.18\n",
    "       Added float values for fractions.\n",
    "\n",
    "min_samples_leaf : int or float, default=1\n",
    "    The minimum number of samples required to be at a leaf node.\n",
    "    A split point at any depth will only be considered if it leaves at\n",
    "    least ``min_samples_leaf`` training samples in each of the left and\n",
    "    right branches.  This may have the effect of smoothing the model,\n",
    "    especially in regression.\n",
    "\n",
    "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "    - If float, then `min_samples_leaf` is a fraction and\n",
    "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "      number of samples for each node.\n",
    "\n",
    "    .. versionchanged:: 0.18\n",
    "       Added float values for fractions.\n",
    "\n",
    "min_weight_fraction_leaf : float, default=0.0\n",
    "    The minimum weighted fraction of the sum total of weights (of all\n",
    "    the input samples) required to be at a leaf node. Samples have\n",
    "    equal weight when sample_weight is not provided.\n",
    "\n",
    "max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
    "    The number of features to consider when looking for the best split:\n",
    "\n",
    "    - If int, then consider `max_features` features at each split.\n",
    "    - If float, then `max_features` is a fraction and\n",
    "      `max(1, int(max_features * n_features_in_))` features are considered at each\n",
    "      split.\n",
    "    - If \"auto\", then `max_features=n_features`.\n",
    "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "    - If \"log2\", then `max_features=log2(n_features)`.\n",
    "    - If None or 1.0, then `max_features=n_features`.\n",
    "\n",
    "    .. note::\n",
    "        The default of 1.0 is equivalent to bagged trees and more\n",
    "        randomness can be achieved by setting smaller values, e.g. 0.3.\n",
    "\n",
    "    .. versionchanged:: 1.1\n",
    "        The default of `max_features` changed from `\"auto\"` to 1.0.\n",
    "\n",
    "    .. deprecated:: 1.1\n",
    "        The `\"auto\"` option was deprecated in 1.1 and will be removed\n",
    "        in 1.3.\n",
    "\n",
    "    Note: the search for a split does not stop until at least one\n",
    "    valid partition of the node samples is found, even if it requires to\n",
    "    effectively inspect more than ``max_features`` features.\n",
    "\n",
    "max_leaf_nodes : int, default=None\n",
    "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
    "    Best nodes are defined as relative reduction in impurity.\n",
    "    If None then unlimited number of leaf nodes.\n",
    "\n",
    "min_impurity_decrease : float, default=0.0\n",
    "    A node will be split if this split induces a decrease of the impurity\n",
    "    greater than or equal to this value.\n",
    "\n",
    "    The weighted impurity decrease equation is the following::\n",
    "\n",
    "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                            - N_t_L / N_t * left_impurity)\n",
    "\n",
    "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "\n",
    "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "    if ``sample_weight`` is passed.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "\n",
    "bootstrap : bool, default=True\n",
    "    Whether bootstrap samples are used when building trees. If False, the\n",
    "    whole dataset is used to build each tree.\n",
    "\n",
    "oob_score : bool, default=False\n",
    "    Whether to use out-of-bag samples to estimate the generalization score.\n",
    "    Only available if bootstrap=True.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
    "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
    "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors. See :term:`Glossary\n",
    "    <n_jobs>` for more details.\n",
    "\n",
    "random_state : int, RandomState instance or None, default=None\n",
    "    Controls both the randomness of the bootstrapping of the samples used\n",
    "    when building trees (if ``bootstrap=True``) and the sampling of the\n",
    "    features to consider when looking for the best split at each node\n",
    "    (if ``max_features < n_features``).\n",
    "    See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "verbose : int, default=0\n",
    "    Controls the verbosity when fitting and predicting.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to ``True``, reuse the solution of the previous call to fit\n",
    "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
    "    new forest. See :term:`Glossary <warm_start>` and\n",
    "    :ref:`gradient_boosting_warm_start` for details.\n",
    "\n",
    "ccp_alpha : non-negative float, default=0.0\n",
    "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
    "    subtree with the largest cost complexity that is smaller than\n",
    "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
    "    :ref:`minimal_cost_complexity_pruning` for details.\n",
    "\n",
    "    .. versionadded:: 0.22\n",
    "\n",
    "max_samples : int or float, default=None\n",
    "    If bootstrap is True, the number of samples to draw from X\n",
    "    to train each base estimator.\n",
    "\n",
    "    - If None (default), then draw `X.shape[0]` samples.\n",
    "    - If int, then draw `max_samples` samples.\n",
    "    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
    "      `max_samples` should be in the interval `(0.0, 1.0]`.\n",
    "\n",
    "    .. versionadded:: 0.22\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`\n",
    "    The child estimator template used to create the collection of fitted\n",
    "    sub-estimators.\n",
    "\n",
    "    .. versionadded:: 1.2\n",
    "       `base_estimator_` was renamed to `estimator_`.\n",
    "\n",
    "base_estimator_ : DecisionTreeRegressor\n",
    "    The child estimator template used to create the collection of fitted\n",
    "    sub-estimators.\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "        `base_estimator_` is deprecated and will be removed in 1.4.\n",
    "        Use `estimator_` instead.\n",
    "\n",
    "estimators_ : list of DecisionTreeRegressor\n",
    "    The collection of fitted sub-estimators.\n",
    "\n",
    "feature_importances_ : ndarray of shape (n_features,)\n",
    "    The impurity-based feature importances.\n",
    "    The higher, the more important the feature.\n",
    "    The importance of a feature is computed as the (normalized)\n",
    "    total reduction of the criterion brought by that feature.  It is also\n",
    "    known as the Gini importance.\n",
    "\n",
    "    Warning: impurity-based feature importances can be misleading for\n",
    "    high cardinality features (many unique values). See\n",
    "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_outputs_ : int\n",
    "    The number of outputs when ``fit`` is performed.\n",
    "\n",
    "oob_score_ : float\n",
    "    Score of the training dataset obtained using an out-of-bag estimate.\n",
    "    This attribute exists only when ``oob_score`` is True.\n",
    "\n",
    "oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Prediction computed with out-of-bag estimate on the training set.\n",
    "    This attribute exists only when ``oob_score`` is True.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
    "sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
    "    tree regressors.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The default values for the parameters controlling the size of the trees\n",
    "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "unpruned trees which can potentially be very large on some data sets. To\n",
    "reduce memory consumption, the complexity and size of the trees should be\n",
    "controlled by setting those parameter values.\n",
    "\n",
    "The features are always randomly permuted at each split. Therefore,\n",
    "the best found split may vary, even with the same training data,\n",
    "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
    "of the criterion is identical for several splits enumerated during the\n",
    "search of the best split. To obtain a deterministic behaviour during\n",
    "fitting, ``random_state`` has to be fixed.\n",
    "\n",
    "The default value ``max_features=\"auto\"`` uses ``n_features``\n",
    "rather than ``n_features / 3``. The latter was originally suggested in\n",
    "[1], whereas the former was more recently justified empirically in [2].\n",
    "\n",
    "References\n",
    "----------\n",
    ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
    "\n",
    ".. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
    "       trees\", Machine Learning, 63(1), 3-42, 2006.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.ensemble import RandomForestRegressor\n",
    ">>> from sklearn.datasets import make_regression\n",
    ">>> X, y = make_regression(n_features=4, n_informative=2,\n",
    "...                        random_state=0, shuffle=False)\n",
    ">>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    ">>> regr.fit(X, y)\n",
    "RandomForestRegressor(...)\n",
    ">>> print(regr.predict([[0, 0, 0, 0]]))\n",
    "[-8.32987858]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_selection._rfe.RFE</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Feature ranking with recursive feature elimination.\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the\n",
    "coefficients of a linear model), the goal of recursive feature elimination\n",
    "(RFE) is to select features by recursively considering smaller and smaller\n",
    "sets of features. First, the estimator is trained on the initial set of\n",
    "features and the importance of each feature is obtained either through\n",
    "any specific attribute or callable.\n",
    "Then, the least important features are pruned from current set of features.\n",
    "That procedure is recursively repeated on the pruned set until the desired\n",
    "number of features to select is eventually reached.\n",
    "\n",
    "Read more in the :ref:`User Guide <rfe>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "estimator : ``Estimator`` instance\n",
    "    A supervised learning estimator with a ``fit`` method that provides\n",
    "    information about feature importance\n",
    "    (e.g. `coef_`, `feature_importances_`).\n",
    "\n",
    "n_features_to_select : int or float, default=None\n",
    "    The number of features to select. If `None`, half of the features are\n",
    "    selected. If integer, the parameter is the absolute number of features\n",
    "    to select. If float between 0 and 1, it is the fraction of features to\n",
    "    select.\n",
    "\n",
    "    .. versionchanged:: 0.24\n",
    "       Added float values for fractions.\n",
    "\n",
    "step : int or float, default=1\n",
    "    If greater than or equal to 1, then ``step`` corresponds to the\n",
    "    (integer) number of features to remove at each iteration.\n",
    "    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n",
    "    (rounded down) of features to remove at each iteration.\n",
    "\n",
    "verbose : int, default=0\n",
    "    Controls verbosity of output.\n",
    "\n",
    "importance_getter : str or callable, default='auto'\n",
    "    If 'auto', uses the feature importance either through a `coef_`\n",
    "    or `feature_importances_` attributes of estimator.\n",
    "\n",
    "    Also accepts a string that specifies an attribute name/path\n",
    "    for extracting feature importance (implemented with `attrgetter`).\n",
    "    For example, give `regressor_.coef_` in case of\n",
    "    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n",
    "    `named_steps.clf.feature_importances_` in case of\n",
    "    class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n",
    "\n",
    "    If `callable`, overrides the default feature importance getter.\n",
    "    The callable is passed with the fitted estimator and it should\n",
    "    return importance for each feature.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "classes_ : ndarray of shape (n_classes,)\n",
    "    The classes labels. Only available when `estimator` is a classifier.\n",
    "\n",
    "estimator_ : ``Estimator`` instance\n",
    "    The fitted estimator used to select features.\n",
    "\n",
    "n_features_ : int\n",
    "    The number of selected features.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`. Only defined if the\n",
    "    underlying estimator exposes such an attribute when fit.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "ranking_ : ndarray of shape (n_features,)\n",
    "    The feature ranking, such that ``ranking_[i]`` corresponds to the\n",
    "    ranking position of the i-th feature. Selected (i.e., estimated\n",
    "    best) features are assigned rank 1.\n",
    "\n",
    "support_ : ndarray of shape (n_features,)\n",
    "    The mask of selected features.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "RFECV : Recursive feature elimination with built-in cross-validated\n",
    "    selection of the best number of features.\n",
    "SelectFromModel : Feature selection based on thresholds of importance\n",
    "    weights.\n",
    "SequentialFeatureSelector : Sequential cross-validation based feature\n",
    "    selection. Does not rely on importance weights.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Allows NaN/Inf in the input if the underlying estimator does as well.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    ".. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n",
    "       for cancer classification using support vector machines\",\n",
    "       Mach. Learn., 46(1-3), 389--422, 2002.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "The following example shows how to retrieve the 5 most informative\n",
    "features in the Friedman #1 dataset.\n",
    "\n",
    ">>> from sklearn.datasets import make_friedman1\n",
    ">>> from sklearn.feature_selection import RFE\n",
    ">>> from sklearn.svm import SVR\n",
    ">>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    ">>> estimator = SVR(kernel=\"linear\")\n",
    ">>> selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    ">>> selector = selector.fit(X, y)\n",
    ">>> selector.support_\n",
    "array([ True,  True,  True,  True,  True, False, False, False, False,\n",
    "       False])\n",
    ">>> selector.ranking_\n",
    "array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_selection._rfe.RFE.fit</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Fit the RFE model and then the underlying estimator on the selected features.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    The training input samples.\n",
    "\n",
    "y : array-like of shape (n_samples,)\n",
    "    The target values.\n",
    "\n",
    "**fit_params : dict\n",
    "    Additional parameters passed to the `fit` method of the underlying\n",
    "    estimator.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "self : object\n",
    "    Fitted estimator.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_selection._rfe.RFE.predict</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Reduce X to the selected features and predict using the estimator.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : array of shape [n_samples, n_features]\n",
    "    The input samples.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "y : array of shape [n_samples]\n",
    "    The predicted target values.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._base.LinearModel.predict</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Predict using the linear model.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "    Samples.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "C : array, shape (n_samples,)\n",
    "    Returns predicted values.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._base.LinearRegression</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Ordinary least squares Linear Regression.\n",
    "\n",
    "LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
    "to minimize the residual sum of squares between the observed targets in\n",
    "the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "fit_intercept : bool, default=True\n",
    "    Whether to calculate the intercept for this model. If set\n",
    "    to False, no intercept will be used in calculations\n",
    "    (i.e. data is expected to be centered).\n",
    "\n",
    "copy_X : bool, default=True\n",
    "    If True, X will be copied; else, it may be overwritten.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    The number of jobs to use for the computation. This will only provide\n",
    "    speedup in case of sufficiently large problems, that is if firstly\n",
    "    `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n",
    "    to `True`. ``None`` means 1 unless in a\n",
    "    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
    "    processors. See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "positive : bool, default=False\n",
    "    When set to ``True``, forces the coefficients to be positive. This\n",
    "    option is only supported for dense arrays.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
    "    Estimated coefficients for the linear regression problem.\n",
    "    If multiple targets are passed during the fit (y 2D), this\n",
    "    is a 2D array of shape (n_targets, n_features), while if only\n",
    "    one target is passed, this is a 1D array of length n_features.\n",
    "\n",
    "rank_ : int\n",
    "    Rank of matrix `X`. Only available when `X` is dense.\n",
    "\n",
    "singular_ : array of shape (min(X, y),)\n",
    "    Singular values of `X`. Only available when `X` is dense.\n",
    "\n",
    "intercept_ : float or array of shape (n_targets,)\n",
    "    Independent term in the linear model. Set to 0.0 if\n",
    "    `fit_intercept = False`.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Ridge : Ridge regression addresses some of the\n",
    "    problems of Ordinary Least Squares by imposing a penalty on the\n",
    "    size of the coefficients with l2 regularization.\n",
    "Lasso : The Lasso is a linear model that estimates\n",
    "    sparse coefficients with l1 regularization.\n",
    "ElasticNet : Elastic-Net is a linear regression\n",
    "    model trained with both l1 and l2 -norm regularization of the\n",
    "    coefficients.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "From the implementation point of view, this is just plain Ordinary\n",
    "Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n",
    "(scipy.optimize.nnls) wrapped as a predictor object.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> import numpy as np\n",
    ">>> from sklearn.linear_model import LinearRegression\n",
    ">>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    ">>> # y = 1 * x_0 + 2 * x_1 + 3\n",
    ">>> y = np.dot(X, np.array([1, 2])) + 3\n",
    ">>> reg = LinearRegression().fit(X, y)\n",
    ">>> reg.score(X, y)\n",
    "1.0\n",
    ">>> reg.coef_\n",
    "array([1., 2.])\n",
    ">>> reg.intercept_\n",
    "3.0...\n",
    ">>> reg.predict(np.array([[3, 5]]))\n",
    "array([16.])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._base.LinearRegression.fit</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Fit linear model.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    Training data.\n",
    "\n",
    "y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
    "    Target values. Will be cast to X's dtype if necessary.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Individual weights for each sample.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       parameter *sample_weight* support to LinearRegression.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "self : object\n",
    "    Fitted Estimator.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.mean_squared_error</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Mean squared error regression loss.\n",
    "\n",
    "Read more in the :ref:`User Guide <mean_squared_error>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
    "    Defines aggregating of multiple output values.\n",
    "    Array-like value defines weights used to average errors.\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of errors in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Errors of all outputs are averaged with uniform weight.\n",
    "\n",
    "squared : bool, default=True\n",
    "    If True returns MSE value, if False returns RMSE value.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "loss : float or ndarray of floats\n",
    "    A non-negative floating point value (the best value is 0.0), or an\n",
    "    array of floating point values, one for each individual target.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import mean_squared_error\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.375\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.612...\n",
    ">>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
    ">>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.708...\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.822...\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "array([0.41666667, 1.        ])\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
    "0.825...\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.r2_score</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    ":math:`R^2` (coefficient of determination) regression score function.\n",
    "\n",
    "Best possible score is 1.0 and it can be negative (because the\n",
    "model can be arbitrarily worse). In the general case when the true y is\n",
    "non-constant, a constant model that always predicts the average y\n",
    "disregarding the input features would get a :math:`R^2` score of 0.0.\n",
    "\n",
    "In the particular case when ``y_true`` is constant, the :math:`R^2` score\n",
    "is not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``\n",
    "(imperfect predictions). To prevent such non-finite numbers to pollute\n",
    "higher-level experiments such as a grid search cross-validation, by default\n",
    "these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect\n",
    "predictions) respectively. You can set ``force_finite`` to ``False`` to\n",
    "prevent this fix from happening.\n",
    "\n",
    "Note: when the prediction residuals have zero mean, the :math:`R^2` score\n",
    "is identical to the\n",
    ":func:`Explained Variance score <explained_variance_score>`.\n",
    "\n",
    "Read more in the :ref:`User Guide <r2_score>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n",
    "\n",
    "    Defines aggregating of multiple output scores.\n",
    "    Array-like value defines weights used to average scores.\n",
    "    Default is \"uniform_average\".\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of scores in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Scores of all outputs are averaged with uniform weight.\n",
    "\n",
    "    'variance_weighted' :\n",
    "        Scores of all outputs are averaged, weighted by the variances\n",
    "        of each individual output.\n",
    "\n",
    "    .. versionchanged:: 0.19\n",
    "        Default value of multioutput is 'uniform_average'.\n",
    "\n",
    "force_finite : bool, default=True\n",
    "    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n",
    "    data should be replaced with real numbers (``1.0`` if prediction is\n",
    "    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n",
    "    for hyperparameters' search procedures (e.g. grid search\n",
    "    cross-validation).\n",
    "\n",
    "    .. versionadded:: 1.1\n",
    "\n",
    "Returns\n",
    "-------\n",
    "z : float or ndarray of floats\n",
    "    The :math:`R^2` score or ndarray of scores if 'multioutput' is\n",
    "    'raw_values'.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "This is not a symmetric function.\n",
    "\n",
    "Unlike most other scores, :math:`R^2` score may be negative (it need not\n",
    "actually be the square of a quantity R).\n",
    "\n",
    "This metric is not well-defined for single samples and will return a NaN\n",
    "value if n_samples is less than two.\n",
    "\n",
    "References\n",
    "----------\n",
    ".. [1] `Wikipedia entry on the Coefficient of determination\n",
    "        <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import r2_score\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.948...\n",
    ">>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    ">>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    ">>> r2_score(y_true, y_pred,\n",
    "...          multioutput='variance_weighted')\n",
    "0.938...\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [1, 2, 3]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "1.0\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [2, 2, 2]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.0\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [3, 2, 1]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "-3.0\n",
    ">>> y_true = [-2, -2, -2]\n",
    ">>> y_pred = [-2, -2, -2]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "1.0\n",
    ">>> r2_score(y_true, y_pred, force_finite=False)\n",
    "nan\n",
    ">>> y_true = [-2, -2, -2]\n",
    ">>> y_pred = [-2, -2, -2 + 1e-8]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.0\n",
    ">>> r2_score(y_true, y_pred, force_finite=False)\n",
    "-inf\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e868be2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>1. Library Loading</h3>  <a id='1'></a><small><a href='#top_phases'>back to top</a></small> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import custom_funcs as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7101fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = ['FPV', 'ATV', 'IDV', 'LPV', 'NFV', 'SQV', 'TPV', 'DRV',]\n",
    "drug = drugs[0]\n",
    "protein = 'protease'\n",
    "\n",
    "data, feat_cols = cf.get_cleaned_data(protein, drug)\n",
    "\n",
    "# Just checking:\n",
    "cf.test_data_integrity(data)\n",
    "\n",
    "## Now, let's do data transformations.\n",
    "data_numeric = cf.to_numeric_rep(data, feat_cols, rep='pKa')\n",
    "\n",
    "# Finally, split the data into a training set, and test set.\n",
    "X, Y, X_train, X_test, Y_train, Y_test = cf.to_train_test_split(data_numeric, feat_cols, drug, test_size=0.3)\n",
    "# sscv = ShuffleSplit(n=len(X_train), n_iter=3, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6363428",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6573406",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "len(data_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c365ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>5. Model Building and Training</h3>  <a id='5'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.ensemble._forest.RandomForestRegressor</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "A random forest regressor.\n",
    "\n",
    "A random forest is a meta estimator that fits a number of classifying\n",
    "decision trees on various sub-samples of the dataset and uses averaging\n",
    "to improve the predictive accuracy and control over-fitting.\n",
    "The sub-sample size is controlled with the `max_samples` parameter if\n",
    "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
    "each tree.\n",
    "\n",
    "Read more in the :ref:`User Guide <forest>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "n_estimators : int, default=100\n",
    "    The number of trees in the forest.\n",
    "\n",
    "    .. versionchanged:: 0.22\n",
    "       The default value of ``n_estimators`` changed from 10 to 100\n",
    "       in 0.22.\n",
    "\n",
    "criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"\n",
    "    The function to measure the quality of a split. Supported criteria\n",
    "    are \"squared_error\" for the mean squared error, which is equal to\n",
    "    variance reduction as feature selection criterion and minimizes the L2\n",
    "    loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
    "    mean squared error with Friedman's improvement score for potential\n",
    "    splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
    "    the L1 loss using the median of each terminal node, and \"poisson\" which\n",
    "    uses reduction in Poisson deviance to find splits.\n",
    "    Training using \"absolute_error\" is significantly slower\n",
    "    than when using \"squared_error\".\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Mean Absolute Error (MAE) criterion.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "       Poisson criterion.\n",
    "\n",
    "max_depth : int, default=None\n",
    "    The maximum depth of the tree. If None, then nodes are expanded until\n",
    "    all leaves are pure or until all leaves contain less than\n",
    "    min_samples_split samples.\n",
    "\n",
    "min_samples_split : int or float, default=2\n",
    "    The minimum number of samples required to split an internal node:\n",
    "\n",
    "    - If int, then consider `min_samples_split` as the minimum number.\n",
    "    - If float, then `min_samples_split` is a fraction and\n",
    "      `ceil(min_samples_split * n_samples)` are the minimum\n",
    "      number of samples for each split.\n",
    "\n",
    "    .. versionchanged:: 0.18\n",
    "       Added float values for fractions.\n",
    "\n",
    "min_samples_leaf : int or float, default=1\n",
    "    The minimum number of samples required to be at a leaf node.\n",
    "    A split point at any depth will only be considered if it leaves at\n",
    "    least ``min_samples_leaf`` training samples in each of the left and\n",
    "    right branches.  This may have the effect of smoothing the model,\n",
    "    especially in regression.\n",
    "\n",
    "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "    - If float, then `min_samples_leaf` is a fraction and\n",
    "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "      number of samples for each node.\n",
    "\n",
    "    .. versionchanged:: 0.18\n",
    "       Added float values for fractions.\n",
    "\n",
    "min_weight_fraction_leaf : float, default=0.0\n",
    "    The minimum weighted fraction of the sum total of weights (of all\n",
    "    the input samples) required to be at a leaf node. Samples have\n",
    "    equal weight when sample_weight is not provided.\n",
    "\n",
    "max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
    "    The number of features to consider when looking for the best split:\n",
    "\n",
    "    - If int, then consider `max_features` features at each split.\n",
    "    - If float, then `max_features` is a fraction and\n",
    "      `max(1, int(max_features * n_features_in_))` features are considered at each\n",
    "      split.\n",
    "    - If \"auto\", then `max_features=n_features`.\n",
    "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "    - If \"log2\", then `max_features=log2(n_features)`.\n",
    "    - If None or 1.0, then `max_features=n_features`.\n",
    "\n",
    "    .. note::\n",
    "        The default of 1.0 is equivalent to bagged trees and more\n",
    "        randomness can be achieved by setting smaller values, e.g. 0.3.\n",
    "\n",
    "    .. versionchanged:: 1.1\n",
    "        The default of `max_features` changed from `\"auto\"` to 1.0.\n",
    "\n",
    "    .. deprecated:: 1.1\n",
    "        The `\"auto\"` option was deprecated in 1.1 and will be removed\n",
    "        in 1.3.\n",
    "\n",
    "    Note: the search for a split does not stop until at least one\n",
    "    valid partition of the node samples is found, even if it requires to\n",
    "    effectively inspect more than ``max_features`` features.\n",
    "\n",
    "max_leaf_nodes : int, default=None\n",
    "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
    "    Best nodes are defined as relative reduction in impurity.\n",
    "    If None then unlimited number of leaf nodes.\n",
    "\n",
    "min_impurity_decrease : float, default=0.0\n",
    "    A node will be split if this split induces a decrease of the impurity\n",
    "    greater than or equal to this value.\n",
    "\n",
    "    The weighted impurity decrease equation is the following::\n",
    "\n",
    "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                            - N_t_L / N_t * left_impurity)\n",
    "\n",
    "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "\n",
    "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "    if ``sample_weight`` is passed.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "\n",
    "bootstrap : bool, default=True\n",
    "    Whether bootstrap samples are used when building trees. If False, the\n",
    "    whole dataset is used to build each tree.\n",
    "\n",
    "oob_score : bool, default=False\n",
    "    Whether to use out-of-bag samples to estimate the generalization score.\n",
    "    Only available if bootstrap=True.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
    "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
    "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors. See :term:`Glossary\n",
    "    <n_jobs>` for more details.\n",
    "\n",
    "random_state : int, RandomState instance or None, default=None\n",
    "    Controls both the randomness of the bootstrapping of the samples used\n",
    "    when building trees (if ``bootstrap=True``) and the sampling of the\n",
    "    features to consider when looking for the best split at each node\n",
    "    (if ``max_features < n_features``).\n",
    "    See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "verbose : int, default=0\n",
    "    Controls the verbosity when fitting and predicting.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to ``True``, reuse the solution of the previous call to fit\n",
    "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
    "    new forest. See :term:`Glossary <warm_start>` and\n",
    "    :ref:`gradient_boosting_warm_start` for details.\n",
    "\n",
    "ccp_alpha : non-negative float, default=0.0\n",
    "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
    "    subtree with the largest cost complexity that is smaller than\n",
    "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
    "    :ref:`minimal_cost_complexity_pruning` for details.\n",
    "\n",
    "    .. versionadded:: 0.22\n",
    "\n",
    "max_samples : int or float, default=None\n",
    "    If bootstrap is True, the number of samples to draw from X\n",
    "    to train each base estimator.\n",
    "\n",
    "    - If None (default), then draw `X.shape[0]` samples.\n",
    "    - If int, then draw `max_samples` samples.\n",
    "    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
    "      `max_samples` should be in the interval `(0.0, 1.0]`.\n",
    "\n",
    "    .. versionadded:: 0.22\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`\n",
    "    The child estimator template used to create the collection of fitted\n",
    "    sub-estimators.\n",
    "\n",
    "    .. versionadded:: 1.2\n",
    "       `base_estimator_` was renamed to `estimator_`.\n",
    "\n",
    "base_estimator_ : DecisionTreeRegressor\n",
    "    The child estimator template used to create the collection of fitted\n",
    "    sub-estimators.\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "        `base_estimator_` is deprecated and will be removed in 1.4.\n",
    "        Use `estimator_` instead.\n",
    "\n",
    "estimators_ : list of DecisionTreeRegressor\n",
    "    The collection of fitted sub-estimators.\n",
    "\n",
    "feature_importances_ : ndarray of shape (n_features,)\n",
    "    The impurity-based feature importances.\n",
    "    The higher, the more important the feature.\n",
    "    The importance of a feature is computed as the (normalized)\n",
    "    total reduction of the criterion brought by that feature.  It is also\n",
    "    known as the Gini importance.\n",
    "\n",
    "    Warning: impurity-based feature importances can be misleading for\n",
    "    high cardinality features (many unique values). See\n",
    "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_outputs_ : int\n",
    "    The number of outputs when ``fit`` is performed.\n",
    "\n",
    "oob_score_ : float\n",
    "    Score of the training dataset obtained using an out-of-bag estimate.\n",
    "    This attribute exists only when ``oob_score`` is True.\n",
    "\n",
    "oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Prediction computed with out-of-bag estimate on the training set.\n",
    "    This attribute exists only when ``oob_score`` is True.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
    "sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
    "    tree regressors.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The default values for the parameters controlling the size of the trees\n",
    "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "unpruned trees which can potentially be very large on some data sets. To\n",
    "reduce memory consumption, the complexity and size of the trees should be\n",
    "controlled by setting those parameter values.\n",
    "\n",
    "The features are always randomly permuted at each split. Therefore,\n",
    "the best found split may vary, even with the same training data,\n",
    "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
    "of the criterion is identical for several splits enumerated during the\n",
    "search of the best split. To obtain a deterministic behaviour during\n",
    "fitting, ``random_state`` has to be fixed.\n",
    "\n",
    "The default value ``max_features=\"auto\"`` uses ``n_features``\n",
    "rather than ``n_features / 3``. The latter was originally suggested in\n",
    "[1], whereas the former was more recently justified empirically in [2].\n",
    "\n",
    "References\n",
    "----------\n",
    ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
    "\n",
    ".. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
    "       trees\", Machine Learning, 63(1), 3-42, 2006.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.ensemble import RandomForestRegressor\n",
    ">>> from sklearn.datasets import make_regression\n",
    ">>> X, y = make_regression(n_features=4, n_informative=2,\n",
    "...                        random_state=0, shuffle=False)\n",
    ">>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    ">>> regr.fit(X, y)\n",
    "RandomForestRegressor(...)\n",
    ">>> print(regr.predict([[0, 0, 0, 0]]))\n",
    "[-8.32987858]\n",
    "\n",
    "</code>\n",
    "<a href='#5'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.ensemble._forest.BaseForest.fit</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Build a forest of trees from the training set (X, y).\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    The training input samples. Internally, its dtype will be converted\n",
    "    to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "    converted into a sparse ``csc_matrix``.\n",
    "\n",
    "y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    The target values (class labels in classification, real numbers in\n",
    "    regression).\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights. If None, then samples are equally weighted. Splits\n",
    "    that would create child nodes with net zero or negative weight are\n",
    "    ignored while searching for a split in each node. In the case of\n",
    "    classification, splits are also ignored if they would result in any\n",
    "    single class carrying a negative weight in either child node.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "self : object\n",
    "    Fitted estimator.\n",
    "\n",
    "</code>\n",
    "<a href='#5'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e79ef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "rfr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc084d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>6. Model Building and Training</h3>  <a id='6'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.ensemble._forest.ForestRegressor.predict</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Predict regression target for X.\n",
    "\n",
    "The predicted regression target of an input sample is computed as the\n",
    "mean predicted regression targets of the trees in the forest.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    The input samples. Internally, its dtype will be converted to\n",
    "    ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "    converted into a sparse ``csr_matrix``.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    The predicted values.\n",
    "\n",
    "</code>\n",
    "<a href='#6'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7687300",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "preds = rfr.predict(X_test)\n",
    "preds[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62175eeb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>7. Library Loading | Visualization</h3>  <a id='7'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>matplotlib</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot.scatter</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x, y : float or array-like, shape (n, )\n",
    "    The data positions.\n",
    "\n",
    "s : float or array-like, shape (n, ), optional\n",
    "    The marker size in points**2 (typographic points are 1/72 in.).\n",
    "    Default is ``rcParams['lines.markersize'] ** 2``.\n",
    "\n",
    "    The linewidth and edgecolor can visually interact with the marker\n",
    "    size, and can lead to artifacts if the marker size is smaller than\n",
    "    the linewidth.\n",
    "\n",
    "    If the linewidth is greater than 0 and the edgecolor is anything\n",
    "    but *'none'*, then the effective size of the marker will be\n",
    "    increased by half the linewidth because the stroke will be centered\n",
    "    on the edge of the shape.\n",
    "\n",
    "    To eliminate the marker edge either set *linewidth=0* or\n",
    "    *edgecolor='none'*.\n",
    "\n",
    "c : array-like or list of colors or color, optional\n",
    "    The marker colors. Possible values:\n",
    "\n",
    "    - A scalar or sequence of n numbers to be mapped to colors using\n",
    "      *cmap* and *norm*.\n",
    "    - A 2D array in which the rows are RGB or RGBA.\n",
    "    - A sequence of colors of length n.\n",
    "    - A single color format string.\n",
    "\n",
    "    Note that *c* should not be a single numeric RGB or RGBA sequence\n",
    "    because that is indistinguishable from an array of values to be\n",
    "    colormapped. If you want to specify the same RGB or RGBA value for\n",
    "    all points, use a 2D array with a single row.  Otherwise,\n",
    "    value-matching will have precedence in case of a size matching with\n",
    "    *x* and *y*.\n",
    "\n",
    "    If you wish to specify a single color for all points\n",
    "    prefer the *color* keyword argument.\n",
    "\n",
    "    Defaults to `None`. In that case the marker color is determined\n",
    "    by the value of *color*, *facecolor* or *facecolors*. In case\n",
    "    those are not specified or `None`, the marker color is determined\n",
    "    by the next color of the ``Axes``' current \"shape and fill\" color\n",
    "    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n",
    "\n",
    "marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n",
    "    The marker style. *marker* can be either an instance of the class\n",
    "    or the text shorthand for a particular marker.\n",
    "    See :mod:`matplotlib.markers` for more information about marker\n",
    "    styles.\n",
    "\n",
    "cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
    "    The Colormap instance or registered colormap name used to map scalar data\n",
    "    to colors.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "norm : str or `~matplotlib.colors.Normalize`, optional\n",
    "    The normalization method used to scale scalar data to the [0, 1] range\n",
    "    before mapping to colors using *cmap*. By default, a linear scaling is\n",
    "    used, mapping the lowest value to 0 and the highest to 1.\n",
    "\n",
    "    If given, this can be one of the following:\n",
    "\n",
    "    - An instance of `.Normalize` or one of its subclasses\n",
    "      (see :ref:`colormapnorms`).\n",
    "    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n",
    "      list of available scales, call `matplotlib.scale.get_scale_names()`.\n",
    "      In that case, a suitable `.Normalize` subclass is dynamically generated\n",
    "      and instantiated.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "vmin, vmax : float, optional\n",
    "    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n",
    "    the data range that the colormap covers. By default, the colormap covers\n",
    "    the complete value range of the supplied data. It is an error to use\n",
    "    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n",
    "    name together with *vmin*/*vmax* is acceptable).\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "alpha : float, default: None\n",
    "    The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "\n",
    "linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
    "    The linewidth of the marker edges. Note: The default *edgecolors*\n",
    "    is 'face'. You may want to change this as well.\n",
    "\n",
    "edgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n",
    "    The edge color of the marker. Possible values:\n",
    "\n",
    "    - 'face': The edge color will always be the same as the face color.\n",
    "    - 'none': No patch boundary will be drawn.\n",
    "    - A color or sequence of colors.\n",
    "\n",
    "    For non-filled markers, *edgecolors* is ignored. Instead, the color\n",
    "    is determined like with 'face', i.e. from *c*, *colors*, or\n",
    "    *facecolors*.\n",
    "\n",
    "plotnonfinite : bool, default: False\n",
    "    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n",
    "    or ``nan``). If ``True`` the points are drawn with the *bad*\n",
    "    colormap color (see `.Colormap.set_bad`).\n",
    "\n",
    "Returns\n",
    "-------\n",
    "`~matplotlib.collections.PathCollection`\n",
    "\n",
    "Other Parameters\n",
    "----------------\n",
    "data : indexable object, optional\n",
    "    If given, the following parameters also accept a string ``s``, which is\n",
    "    interpreted as ``data[s]`` (unless this raises an exception):\n",
    "\n",
    "    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n",
    "**kwargs : `~matplotlib.collections.Collection` properties\n",
    "\n",
    "See Also\n",
    "--------\n",
    "plot : To plot scatter plots when markers are identical in size and\n",
    "    color.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "* The `.plot` function will be faster for scatterplots where markers\n",
    "  don't vary in size or color.\n",
    "\n",
    "* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n",
    "  case all masks will be combined and only unmasked points will be\n",
    "  plotted.\n",
    "\n",
    "* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n",
    "  may be input as N-D arrays, but within scatter they will be\n",
    "  flattened. The exception is *c*, which will be flattened only if its\n",
    "  size matches the size of *x* and *y*.\n",
    "\n",
    "</code>\n",
    "<a href='#7'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc2f18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(Y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229cf605",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>8. Library Loading | Model Building and Training</h3>  <a id='8'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.r2_score</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    ":math:`R^2` (coefficient of determination) regression score function.\n",
    "\n",
    "Best possible score is 1.0 and it can be negative (because the\n",
    "model can be arbitrarily worse). In the general case when the true y is\n",
    "non-constant, a constant model that always predicts the average y\n",
    "disregarding the input features would get a :math:`R^2` score of 0.0.\n",
    "\n",
    "In the particular case when ``y_true`` is constant, the :math:`R^2` score\n",
    "is not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``\n",
    "(imperfect predictions). To prevent such non-finite numbers to pollute\n",
    "higher-level experiments such as a grid search cross-validation, by default\n",
    "these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect\n",
    "predictions) respectively. You can set ``force_finite`` to ``False`` to\n",
    "prevent this fix from happening.\n",
    "\n",
    "Note: when the prediction residuals have zero mean, the :math:`R^2` score\n",
    "is identical to the\n",
    ":func:`Explained Variance score <explained_variance_score>`.\n",
    "\n",
    "Read more in the :ref:`User Guide <r2_score>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n",
    "\n",
    "    Defines aggregating of multiple output scores.\n",
    "    Array-like value defines weights used to average scores.\n",
    "    Default is \"uniform_average\".\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of scores in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Scores of all outputs are averaged with uniform weight.\n",
    "\n",
    "    'variance_weighted' :\n",
    "        Scores of all outputs are averaged, weighted by the variances\n",
    "        of each individual output.\n",
    "\n",
    "    .. versionchanged:: 0.19\n",
    "        Default value of multioutput is 'uniform_average'.\n",
    "\n",
    "force_finite : bool, default=True\n",
    "    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n",
    "    data should be replaced with real numbers (``1.0`` if prediction is\n",
    "    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n",
    "    for hyperparameters' search procedures (e.g. grid search\n",
    "    cross-validation).\n",
    "\n",
    "    .. versionadded:: 1.1\n",
    "\n",
    "Returns\n",
    "-------\n",
    "z : float or ndarray of floats\n",
    "    The :math:`R^2` score or ndarray of scores if 'multioutput' is\n",
    "    'raw_values'.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "This is not a symmetric function.\n",
    "\n",
    "Unlike most other scores, :math:`R^2` score may be negative (it need not\n",
    "actually be the square of a quantity R).\n",
    "\n",
    "This metric is not well-defined for single samples and will return a NaN\n",
    "value if n_samples is less than two.\n",
    "\n",
    "References\n",
    "----------\n",
    ".. [1] `Wikipedia entry on the Coefficient of determination\n",
    "        <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import r2_score\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.948...\n",
    ">>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    ">>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    ">>> r2_score(y_true, y_pred,\n",
    "...          multioutput='variance_weighted')\n",
    "0.938...\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [1, 2, 3]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "1.0\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [2, 2, 2]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.0\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [3, 2, 1]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "-3.0\n",
    ">>> y_true = [-2, -2, -2]\n",
    ">>> y_pred = [-2, -2, -2]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "1.0\n",
    ">>> r2_score(y_true, y_pred, force_finite=False)\n",
    "nan\n",
    ">>> y_true = [-2, -2, -2]\n",
    ">>> y_pred = [-2, -2, -2 + 1e-8]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.0\n",
    ">>> r2_score(y_true, y_pred, force_finite=False)\n",
    "-inf\n",
    "\n",
    "</code>\n",
    "<a href='#8'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526fbb19",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "r2_score(Y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0cbc70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>9. Model Building and Training</h3>  <a id='9'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.mean_squared_error</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Mean squared error regression loss.\n",
    "\n",
    "Read more in the :ref:`User Guide <mean_squared_error>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
    "    Defines aggregating of multiple output values.\n",
    "    Array-like value defines weights used to average errors.\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of errors in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Errors of all outputs are averaged with uniform weight.\n",
    "\n",
    "squared : bool, default=True\n",
    "    If True returns MSE value, if False returns RMSE value.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "loss : float or ndarray of floats\n",
    "    A non-negative floating point value (the best value is 0.0), or an\n",
    "    array of floating point values, one for each individual target.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import mean_squared_error\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.375\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.612...\n",
    ">>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
    ">>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.708...\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.822...\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "array([0.41666667, 1.        ])\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
    "0.825...\n",
    "\n",
    "</code>\n",
    "<a href='#9'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a3ac9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "mean_squared_error(Y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137688cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>10. Visualization</h3>  <a id='10'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>matplotlib</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot.plot</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Plot y versus x as lines and/or markers.\n",
    "\n",
    "Call signatures::\n",
    "\n",
    "    plot([x], y, [fmt], *, data=None, **kwargs)\n",
    "    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
    "\n",
    "The coordinates of the points or line nodes are given by *x*, *y*.\n",
    "\n",
    "The optional parameter *fmt* is a convenient way for defining basic\n",
    "formatting like color, marker and linestyle. It's a shortcut string\n",
    "notation described in the *Notes* section below.\n",
    "\n",
    ">>> plot(x, y)        # plot x and y using default line style and color\n",
    ">>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n",
    ">>> plot(y)           # plot y using x as index array 0..N-1\n",
    ">>> plot(y, 'r+')     # ditto, but with red plusses\n",
    "\n",
    "You can use `.Line2D` properties as keyword arguments for more\n",
    "control on the appearance. Line properties and *fmt* can be mixed.\n",
    "The following two calls yield identical results:\n",
    "\n",
    ">>> plot(x, y, 'go--', linewidth=2, markersize=12)\n",
    ">>> plot(x, y, color='green', marker='o', linestyle='dashed',\n",
    "...      linewidth=2, markersize=12)\n",
    "\n",
    "When conflicting with *fmt*, keyword arguments take precedence.\n",
    "\n",
    "\n",
    "**Plotting labelled data**\n",
    "\n",
    "There's a convenient way for plotting objects with labelled data (i.e.\n",
    "data that can be accessed by index ``obj['y']``). Instead of giving\n",
    "the data in *x* and *y*, you can provide the object in the *data*\n",
    "parameter and just give the labels for *x* and *y*::\n",
    "\n",
    ">>> plot('xlabel', 'ylabel', data=obj)\n",
    "\n",
    "All indexable objects are supported. This could e.g. be a `dict`, a\n",
    "`pandas.DataFrame` or a structured numpy array.\n",
    "\n",
    "\n",
    "**Plotting multiple sets of data**\n",
    "\n",
    "There are various ways to plot multiple sets of data.\n",
    "\n",
    "- The most straight forward way is just to call `plot` multiple times.\n",
    "  Example:\n",
    "\n",
    "  >>> plot(x1, y1, 'bo')\n",
    "  >>> plot(x2, y2, 'go')\n",
    "\n",
    "- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n",
    "  for every column. If both *x* and *y* are 2D, they must have the\n",
    "  same shape. If only one of them is 2D with shape (N, m) the other\n",
    "  must have length N and will be used for every data set m.\n",
    "\n",
    "  Example:\n",
    "\n",
    "  >>> x = [1, 2, 3]\n",
    "  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "  >>> plot(x, y)\n",
    "\n",
    "  is equivalent to:\n",
    "\n",
    "  >>> for col in range(y.shape[1]):\n",
    "  ...     plot(x, y[:, col])\n",
    "\n",
    "- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n",
    "  groups::\n",
    "\n",
    "  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n",
    "\n",
    "  In this case, any additional keyword argument applies to all\n",
    "  datasets. Also, this syntax cannot be combined with the *data*\n",
    "  parameter.\n",
    "\n",
    "By default, each line is assigned a different style specified by a\n",
    "'style cycle'. The *fmt* and line property parameters are only\n",
    "necessary if you want explicit deviations from these defaults.\n",
    "Alternatively, you can also change the style cycle using\n",
    ":rc:`axes.prop_cycle`.\n",
    "\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x, y : array-like or scalar\n",
    "    The horizontal / vertical coordinates of the data points.\n",
    "    *x* values are optional and default to ``range(len(y))``.\n",
    "\n",
    "    Commonly, these parameters are 1D arrays.\n",
    "\n",
    "    They can also be scalars, or two-dimensional (in that case, the\n",
    "    columns represent separate data sets).\n",
    "\n",
    "    These arguments cannot be passed as keywords.\n",
    "\n",
    "fmt : str, optional\n",
    "    A format string, e.g. 'ro' for red circles. See the *Notes*\n",
    "    section for a full description of the format strings.\n",
    "\n",
    "    Format strings are just an abbreviation for quickly setting\n",
    "    basic line properties. All of these and more can also be\n",
    "    controlled by keyword arguments.\n",
    "\n",
    "    This argument cannot be passed as keyword.\n",
    "\n",
    "data : indexable object, optional\n",
    "    An object with labelled data. If given, provide the label names to\n",
    "    plot in *x* and *y*.\n",
    "\n",
    "    .. note::\n",
    "        Technically there's a slight ambiguity in calls where the\n",
    "        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n",
    "        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n",
    "        the former interpretation is chosen, but a warning is issued.\n",
    "        You may suppress the warning by adding an empty format string\n",
    "        ``plot('n', 'o', '', data=obj)``.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "list of `.Line2D`\n",
    "    A list of lines representing the plotted data.\n",
    "\n",
    "Other Parameters\n",
    "----------------\n",
    "scalex, scaley : bool, default: True\n",
    "    These parameters determine if the view limits are adapted to the\n",
    "    data limits. The values are passed on to\n",
    "    `~.axes.Axes.autoscale_view`.\n",
    "\n",
    "**kwargs : `~matplotlib.lines.Line2D` properties, optional\n",
    "    *kwargs* are used to specify properties like a line label (for\n",
    "    auto legends), linewidth, antialiasing, marker face color.\n",
    "    Example::\n",
    "\n",
    "    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n",
    "    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n",
    "\n",
    "    If you specify multiple lines with one plot call, the kwargs apply\n",
    "    to all those lines. In case the label object is iterable, each\n",
    "    element is used as labels for each set of data.\n",
    "\n",
    "    Here is a list of available `.Line2D` properties:\n",
    "\n",
    "    Properties:\n",
    "    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n",
    "    alpha: scalar or None\n",
    "    animated: bool\n",
    "    antialiased or aa: bool\n",
    "    clip_box: `~matplotlib.transforms.BboxBase` or None\n",
    "    clip_on: bool\n",
    "    clip_path: Patch or (Path, Transform) or None\n",
    "    color or c: color\n",
    "    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n",
    "    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n",
    "    dashes: sequence of floats (on/off ink in points) or (None, None)\n",
    "    data: (2, N) array or two 1D arrays\n",
    "    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
    "    figure: `~matplotlib.figure.Figure`\n",
    "    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
    "    gapcolor: color or None\n",
    "    gid: str\n",
    "    in_layout: bool\n",
    "    label: object\n",
    "    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
    "    linewidth or lw: float\n",
    "    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
    "    markeredgecolor or mec: color\n",
    "    markeredgewidth or mew: float\n",
    "    markerfacecolor or mfc: color\n",
    "    markerfacecoloralt or mfcalt: color\n",
    "    markersize or ms: float\n",
    "    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n",
    "    mouseover: bool\n",
    "    path_effects: list of `.AbstractPathEffect`\n",
    "    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n",
    "    pickradius: float\n",
    "    rasterized: bool\n",
    "    sketch_params: (scale: float, length: float, randomness: float)\n",
    "    snap: bool or None\n",
    "    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n",
    "    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n",
    "    transform: unknown\n",
    "    url: str\n",
    "    visible: bool\n",
    "    xdata: 1D array\n",
    "    ydata: 1D array\n",
    "    zorder: float\n",
    "\n",
    "See Also\n",
    "--------\n",
    "scatter : XY scatter plot with markers of varying size and/or color (\n",
    "    sometimes also called bubble chart).\n",
    "\n",
    "Notes\n",
    "-----\n",
    "**Format Strings**\n",
    "\n",
    "A format string consists of a part for color, marker and line::\n",
    "\n",
    "    fmt = '[marker][line][color]'\n",
    "\n",
    "Each of them is optional. If not provided, the value from the style\n",
    "cycle is used. Exception: If ``line`` is given, but no ``marker``,\n",
    "the data will be a line without markers.\n",
    "\n",
    "Other combinations such as ``[color][marker][line]`` are also\n",
    "supported, but note that their parsing may be ambiguous.\n",
    "\n",
    "**Markers**\n",
    "\n",
    "=============   ===============================\n",
    "character       description\n",
    "=============   ===============================\n",
    "``'.'``         point marker\n",
    "``','``         pixel marker\n",
    "``'o'``         circle marker\n",
    "``'v'``         triangle_down marker\n",
    "``'^'``         triangle_up marker\n",
    "``'<'``         triangle_left marker\n",
    "``'>'``         triangle_right marker\n",
    "``'1'``         tri_down marker\n",
    "``'2'``         tri_up marker\n",
    "``'3'``         tri_left marker\n",
    "``'4'``         tri_right marker\n",
    "``'8'``         octagon marker\n",
    "``'s'``         square marker\n",
    "``'p'``         pentagon marker\n",
    "``'P'``         plus (filled) marker\n",
    "``'*'``         star marker\n",
    "``'h'``         hexagon1 marker\n",
    "``'H'``         hexagon2 marker\n",
    "``'+'``         plus marker\n",
    "``'x'``         x marker\n",
    "``'X'``         x (filled) marker\n",
    "``'D'``         diamond marker\n",
    "``'d'``         thin_diamond marker\n",
    "``'|'``         vline marker\n",
    "``'_'``         hline marker\n",
    "=============   ===============================\n",
    "\n",
    "**Line Styles**\n",
    "\n",
    "=============    ===============================\n",
    "character        description\n",
    "=============    ===============================\n",
    "``'-'``          solid line style\n",
    "``'--'``         dashed line style\n",
    "``'-.'``         dash-dot line style\n",
    "``':'``          dotted line style\n",
    "=============    ===============================\n",
    "\n",
    "Example format strings::\n",
    "\n",
    "    'b'    # blue markers with default shape\n",
    "    'or'   # red circles\n",
    "    '-g'   # green solid line\n",
    "    '--'   # dashed line with default color\n",
    "    '^k:'  # black triangle_up markers connected by a dotted line\n",
    "\n",
    "**Colors**\n",
    "\n",
    "The supported color abbreviations are the single letter codes\n",
    "\n",
    "=============    ===============================\n",
    "character        color\n",
    "=============    ===============================\n",
    "``'b'``          blue\n",
    "``'g'``          green\n",
    "``'r'``          red\n",
    "``'c'``          cyan\n",
    "``'m'``          magenta\n",
    "``'y'``          yellow\n",
    "``'k'``          black\n",
    "``'w'``          white\n",
    "=============    ===============================\n",
    "\n",
    "and the ``'CN'`` colors that index into the default property cycle.\n",
    "\n",
    "If the color is the only part of the format string, you can\n",
    "additionally use any  `matplotlib.colors` spec, e.g. full names\n",
    "(``'green'``) or hex strings (``'#008000'``).\n",
    "\n",
    "</code>\n",
    "<a href='#10'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac33272",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.plot(rfr.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c0309",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>11. Library Loading</h3>  <a id='11'></a><small><a href='#top_phases'>back to top</a></small> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5637f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf544c57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>12. Model Building and Training</h3>  <a id='12'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._base.LinearRegression</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Ordinary least squares Linear Regression.\n",
    "\n",
    "LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
    "to minimize the residual sum of squares between the observed targets in\n",
    "the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "fit_intercept : bool, default=True\n",
    "    Whether to calculate the intercept for this model. If set\n",
    "    to False, no intercept will be used in calculations\n",
    "    (i.e. data is expected to be centered).\n",
    "\n",
    "copy_X : bool, default=True\n",
    "    If True, X will be copied; else, it may be overwritten.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    The number of jobs to use for the computation. This will only provide\n",
    "    speedup in case of sufficiently large problems, that is if firstly\n",
    "    `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n",
    "    to `True`. ``None`` means 1 unless in a\n",
    "    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
    "    processors. See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "positive : bool, default=False\n",
    "    When set to ``True``, forces the coefficients to be positive. This\n",
    "    option is only supported for dense arrays.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
    "    Estimated coefficients for the linear regression problem.\n",
    "    If multiple targets are passed during the fit (y 2D), this\n",
    "    is a 2D array of shape (n_targets, n_features), while if only\n",
    "    one target is passed, this is a 1D array of length n_features.\n",
    "\n",
    "rank_ : int\n",
    "    Rank of matrix `X`. Only available when `X` is dense.\n",
    "\n",
    "singular_ : array of shape (min(X, y),)\n",
    "    Singular values of `X`. Only available when `X` is dense.\n",
    "\n",
    "intercept_ : float or array of shape (n_targets,)\n",
    "    Independent term in the linear model. Set to 0.0 if\n",
    "    `fit_intercept = False`.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Ridge : Ridge regression addresses some of the\n",
    "    problems of Ordinary Least Squares by imposing a penalty on the\n",
    "    size of the coefficients with l2 regularization.\n",
    "Lasso : The Lasso is a linear model that estimates\n",
    "    sparse coefficients with l1 regularization.\n",
    "ElasticNet : Elastic-Net is a linear regression\n",
    "    model trained with both l1 and l2 -norm regularization of the\n",
    "    coefficients.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "From the implementation point of view, this is just plain Ordinary\n",
    "Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n",
    "(scipy.optimize.nnls) wrapped as a predictor object.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> import numpy as np\n",
    ">>> from sklearn.linear_model import LinearRegression\n",
    ">>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    ">>> # y = 1 * x_0 + 2 * x_1 + 3\n",
    ">>> y = np.dot(X, np.array([1, 2])) + 3\n",
    ">>> reg = LinearRegression().fit(X, y)\n",
    ">>> reg.score(X, y)\n",
    "1.0\n",
    ">>> reg.coef_\n",
    "array([1., 2.])\n",
    ">>> reg.intercept_\n",
    "3.0...\n",
    ">>> reg.predict(np.array([[3, 5]]))\n",
    "array([16.])\n",
    "\n",
    "</code>\n",
    "<a href='#12'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._base.LinearRegression.fit</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Fit linear model.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    Training data.\n",
    "\n",
    "y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
    "    Target values. Will be cast to X's dtype if necessary.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Individual weights for each sample.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       parameter *sample_weight* support to LinearRegression.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "self : object\n",
    "    Fitted Estimator.\n",
    "\n",
    "</code>\n",
    "<a href='#12'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._base.LinearModel.predict</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Predict using the linear model.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "    Samples.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "C : array, shape (n_samples,)\n",
    "    Returns predicted values.\n",
    "\n",
    "</code>\n",
    "<a href='#12'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c790ee0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "preds = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd36b52",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>13. Model Building and Training</h3>  <a id='13'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.r2_score</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    ":math:`R^2` (coefficient of determination) regression score function.\n",
    "\n",
    "Best possible score is 1.0 and it can be negative (because the\n",
    "model can be arbitrarily worse). In the general case when the true y is\n",
    "non-constant, a constant model that always predicts the average y\n",
    "disregarding the input features would get a :math:`R^2` score of 0.0.\n",
    "\n",
    "In the particular case when ``y_true`` is constant, the :math:`R^2` score\n",
    "is not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``\n",
    "(imperfect predictions). To prevent such non-finite numbers to pollute\n",
    "higher-level experiments such as a grid search cross-validation, by default\n",
    "these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect\n",
    "predictions) respectively. You can set ``force_finite`` to ``False`` to\n",
    "prevent this fix from happening.\n",
    "\n",
    "Note: when the prediction residuals have zero mean, the :math:`R^2` score\n",
    "is identical to the\n",
    ":func:`Explained Variance score <explained_variance_score>`.\n",
    "\n",
    "Read more in the :ref:`User Guide <r2_score>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n",
    "\n",
    "    Defines aggregating of multiple output scores.\n",
    "    Array-like value defines weights used to average scores.\n",
    "    Default is \"uniform_average\".\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of scores in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Scores of all outputs are averaged with uniform weight.\n",
    "\n",
    "    'variance_weighted' :\n",
    "        Scores of all outputs are averaged, weighted by the variances\n",
    "        of each individual output.\n",
    "\n",
    "    .. versionchanged:: 0.19\n",
    "        Default value of multioutput is 'uniform_average'.\n",
    "\n",
    "force_finite : bool, default=True\n",
    "    Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n",
    "    data should be replaced with real numbers (``1.0`` if prediction is\n",
    "    perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n",
    "    for hyperparameters' search procedures (e.g. grid search\n",
    "    cross-validation).\n",
    "\n",
    "    .. versionadded:: 1.1\n",
    "\n",
    "Returns\n",
    "-------\n",
    "z : float or ndarray of floats\n",
    "    The :math:`R^2` score or ndarray of scores if 'multioutput' is\n",
    "    'raw_values'.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "This is not a symmetric function.\n",
    "\n",
    "Unlike most other scores, :math:`R^2` score may be negative (it need not\n",
    "actually be the square of a quantity R).\n",
    "\n",
    "This metric is not well-defined for single samples and will return a NaN\n",
    "value if n_samples is less than two.\n",
    "\n",
    "References\n",
    "----------\n",
    ".. [1] `Wikipedia entry on the Coefficient of determination\n",
    "        <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import r2_score\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.948...\n",
    ">>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    ">>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    ">>> r2_score(y_true, y_pred,\n",
    "...          multioutput='variance_weighted')\n",
    "0.938...\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [1, 2, 3]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "1.0\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [2, 2, 2]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.0\n",
    ">>> y_true = [1, 2, 3]\n",
    ">>> y_pred = [3, 2, 1]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "-3.0\n",
    ">>> y_true = [-2, -2, -2]\n",
    ">>> y_pred = [-2, -2, -2]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "1.0\n",
    ">>> r2_score(y_true, y_pred, force_finite=False)\n",
    "nan\n",
    ">>> y_true = [-2, -2, -2]\n",
    ">>> y_pred = [-2, -2, -2 + 1e-8]\n",
    ">>> r2_score(y_true, y_pred)\n",
    "0.0\n",
    ">>> r2_score(y_true, y_pred, force_finite=False)\n",
    "-inf\n",
    "\n",
    "</code>\n",
    "<a href='#13'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d11c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "r2_score(Y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb1dd2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>14. Model Building and Training</h3>  <a id='14'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.mean_squared_error</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Mean squared error regression loss.\n",
    "\n",
    "Read more in the :ref:`User Guide <mean_squared_error>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
    "    Defines aggregating of multiple output values.\n",
    "    Array-like value defines weights used to average errors.\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of errors in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Errors of all outputs are averaged with uniform weight.\n",
    "\n",
    "squared : bool, default=True\n",
    "    If True returns MSE value, if False returns RMSE value.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "loss : float or ndarray of floats\n",
    "    A non-negative floating point value (the best value is 0.0), or an\n",
    "    array of floating point values, one for each individual target.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import mean_squared_error\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.375\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.612...\n",
    ">>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
    ">>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.708...\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.822...\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "array([0.41666667, 1.        ])\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
    "0.825...\n",
    "\n",
    "</code>\n",
    "<a href='#14'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a7947",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "mean_squared_error(Y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246da136",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>15. Visualization</h3>  <a id='15'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>matplotlib</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot.scatter</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x, y : float or array-like, shape (n, )\n",
    "    The data positions.\n",
    "\n",
    "s : float or array-like, shape (n, ), optional\n",
    "    The marker size in points**2 (typographic points are 1/72 in.).\n",
    "    Default is ``rcParams['lines.markersize'] ** 2``.\n",
    "\n",
    "    The linewidth and edgecolor can visually interact with the marker\n",
    "    size, and can lead to artifacts if the marker size is smaller than\n",
    "    the linewidth.\n",
    "\n",
    "    If the linewidth is greater than 0 and the edgecolor is anything\n",
    "    but *'none'*, then the effective size of the marker will be\n",
    "    increased by half the linewidth because the stroke will be centered\n",
    "    on the edge of the shape.\n",
    "\n",
    "    To eliminate the marker edge either set *linewidth=0* or\n",
    "    *edgecolor='none'*.\n",
    "\n",
    "c : array-like or list of colors or color, optional\n",
    "    The marker colors. Possible values:\n",
    "\n",
    "    - A scalar or sequence of n numbers to be mapped to colors using\n",
    "      *cmap* and *norm*.\n",
    "    - A 2D array in which the rows are RGB or RGBA.\n",
    "    - A sequence of colors of length n.\n",
    "    - A single color format string.\n",
    "\n",
    "    Note that *c* should not be a single numeric RGB or RGBA sequence\n",
    "    because that is indistinguishable from an array of values to be\n",
    "    colormapped. If you want to specify the same RGB or RGBA value for\n",
    "    all points, use a 2D array with a single row.  Otherwise,\n",
    "    value-matching will have precedence in case of a size matching with\n",
    "    *x* and *y*.\n",
    "\n",
    "    If you wish to specify a single color for all points\n",
    "    prefer the *color* keyword argument.\n",
    "\n",
    "    Defaults to `None`. In that case the marker color is determined\n",
    "    by the value of *color*, *facecolor* or *facecolors*. In case\n",
    "    those are not specified or `None`, the marker color is determined\n",
    "    by the next color of the ``Axes``' current \"shape and fill\" color\n",
    "    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n",
    "\n",
    "marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n",
    "    The marker style. *marker* can be either an instance of the class\n",
    "    or the text shorthand for a particular marker.\n",
    "    See :mod:`matplotlib.markers` for more information about marker\n",
    "    styles.\n",
    "\n",
    "cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
    "    The Colormap instance or registered colormap name used to map scalar data\n",
    "    to colors.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "norm : str or `~matplotlib.colors.Normalize`, optional\n",
    "    The normalization method used to scale scalar data to the [0, 1] range\n",
    "    before mapping to colors using *cmap*. By default, a linear scaling is\n",
    "    used, mapping the lowest value to 0 and the highest to 1.\n",
    "\n",
    "    If given, this can be one of the following:\n",
    "\n",
    "    - An instance of `.Normalize` or one of its subclasses\n",
    "      (see :ref:`colormapnorms`).\n",
    "    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n",
    "      list of available scales, call `matplotlib.scale.get_scale_names()`.\n",
    "      In that case, a suitable `.Normalize` subclass is dynamically generated\n",
    "      and instantiated.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "vmin, vmax : float, optional\n",
    "    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n",
    "    the data range that the colormap covers. By default, the colormap covers\n",
    "    the complete value range of the supplied data. It is an error to use\n",
    "    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n",
    "    name together with *vmin*/*vmax* is acceptable).\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "alpha : float, default: None\n",
    "    The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "\n",
    "linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
    "    The linewidth of the marker edges. Note: The default *edgecolors*\n",
    "    is 'face'. You may want to change this as well.\n",
    "\n",
    "edgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n",
    "    The edge color of the marker. Possible values:\n",
    "\n",
    "    - 'face': The edge color will always be the same as the face color.\n",
    "    - 'none': No patch boundary will be drawn.\n",
    "    - A color or sequence of colors.\n",
    "\n",
    "    For non-filled markers, *edgecolors* is ignored. Instead, the color\n",
    "    is determined like with 'face', i.e. from *c*, *colors*, or\n",
    "    *facecolors*.\n",
    "\n",
    "plotnonfinite : bool, default: False\n",
    "    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n",
    "    or ``nan``). If ``True`` the points are drawn with the *bad*\n",
    "    colormap color (see `.Colormap.set_bad`).\n",
    "\n",
    "Returns\n",
    "-------\n",
    "`~matplotlib.collections.PathCollection`\n",
    "\n",
    "Other Parameters\n",
    "----------------\n",
    "data : indexable object, optional\n",
    "    If given, the following parameters also accept a string ``s``, which is\n",
    "    interpreted as ``data[s]`` (unless this raises an exception):\n",
    "\n",
    "    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n",
    "**kwargs : `~matplotlib.collections.Collection` properties\n",
    "\n",
    "See Also\n",
    "--------\n",
    "plot : To plot scatter plots when markers are identical in size and\n",
    "    color.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "* The `.plot` function will be faster for scatterplots where markers\n",
    "  don't vary in size or color.\n",
    "\n",
    "* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n",
    "  case all masks will be combined and only unmasked points will be\n",
    "  plotted.\n",
    "\n",
    "* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n",
    "  may be input as N-D arrays, but within scatter they will be\n",
    "  flattened. The exception is *c*, which will be flattened only if its\n",
    "  size matches the size of *x* and *y*.\n",
    "\n",
    "</code>\n",
    "<a href='#15'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e74a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c2c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(preds).index(min(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d95d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = list(preds)\n",
    "del preds_list[193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184f10a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "Y_test_list = list(Y_test)\n",
    "del Y_test_list[193]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eea131",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>20. Visualization</h3>  <a id='20'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>matplotlib</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>matplotlib.pyplot.scatter</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x, y : float or array-like, shape (n, )\n",
    "    The data positions.\n",
    "\n",
    "s : float or array-like, shape (n, ), optional\n",
    "    The marker size in points**2 (typographic points are 1/72 in.).\n",
    "    Default is ``rcParams['lines.markersize'] ** 2``.\n",
    "\n",
    "    The linewidth and edgecolor can visually interact with the marker\n",
    "    size, and can lead to artifacts if the marker size is smaller than\n",
    "    the linewidth.\n",
    "\n",
    "    If the linewidth is greater than 0 and the edgecolor is anything\n",
    "    but *'none'*, then the effective size of the marker will be\n",
    "    increased by half the linewidth because the stroke will be centered\n",
    "    on the edge of the shape.\n",
    "\n",
    "    To eliminate the marker edge either set *linewidth=0* or\n",
    "    *edgecolor='none'*.\n",
    "\n",
    "c : array-like or list of colors or color, optional\n",
    "    The marker colors. Possible values:\n",
    "\n",
    "    - A scalar or sequence of n numbers to be mapped to colors using\n",
    "      *cmap* and *norm*.\n",
    "    - A 2D array in which the rows are RGB or RGBA.\n",
    "    - A sequence of colors of length n.\n",
    "    - A single color format string.\n",
    "\n",
    "    Note that *c* should not be a single numeric RGB or RGBA sequence\n",
    "    because that is indistinguishable from an array of values to be\n",
    "    colormapped. If you want to specify the same RGB or RGBA value for\n",
    "    all points, use a 2D array with a single row.  Otherwise,\n",
    "    value-matching will have precedence in case of a size matching with\n",
    "    *x* and *y*.\n",
    "\n",
    "    If you wish to specify a single color for all points\n",
    "    prefer the *color* keyword argument.\n",
    "\n",
    "    Defaults to `None`. In that case the marker color is determined\n",
    "    by the value of *color*, *facecolor* or *facecolors*. In case\n",
    "    those are not specified or `None`, the marker color is determined\n",
    "    by the next color of the ``Axes``' current \"shape and fill\" color\n",
    "    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n",
    "\n",
    "marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n",
    "    The marker style. *marker* can be either an instance of the class\n",
    "    or the text shorthand for a particular marker.\n",
    "    See :mod:`matplotlib.markers` for more information about marker\n",
    "    styles.\n",
    "\n",
    "cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
    "    The Colormap instance or registered colormap name used to map scalar data\n",
    "    to colors.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "norm : str or `~matplotlib.colors.Normalize`, optional\n",
    "    The normalization method used to scale scalar data to the [0, 1] range\n",
    "    before mapping to colors using *cmap*. By default, a linear scaling is\n",
    "    used, mapping the lowest value to 0 and the highest to 1.\n",
    "\n",
    "    If given, this can be one of the following:\n",
    "\n",
    "    - An instance of `.Normalize` or one of its subclasses\n",
    "      (see :ref:`colormapnorms`).\n",
    "    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n",
    "      list of available scales, call `matplotlib.scale.get_scale_names()`.\n",
    "      In that case, a suitable `.Normalize` subclass is dynamically generated\n",
    "      and instantiated.\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "vmin, vmax : float, optional\n",
    "    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n",
    "    the data range that the colormap covers. By default, the colormap covers\n",
    "    the complete value range of the supplied data. It is an error to use\n",
    "    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n",
    "    name together with *vmin*/*vmax* is acceptable).\n",
    "\n",
    "    This parameter is ignored if *c* is RGB(A).\n",
    "\n",
    "alpha : float, default: None\n",
    "    The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
    "\n",
    "linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
    "    The linewidth of the marker edges. Note: The default *edgecolors*\n",
    "    is 'face'. You may want to change this as well.\n",
    "\n",
    "edgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n",
    "    The edge color of the marker. Possible values:\n",
    "\n",
    "    - 'face': The edge color will always be the same as the face color.\n",
    "    - 'none': No patch boundary will be drawn.\n",
    "    - A color or sequence of colors.\n",
    "\n",
    "    For non-filled markers, *edgecolors* is ignored. Instead, the color\n",
    "    is determined like with 'face', i.e. from *c*, *colors*, or\n",
    "    *facecolors*.\n",
    "\n",
    "plotnonfinite : bool, default: False\n",
    "    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n",
    "    or ``nan``). If ``True`` the points are drawn with the *bad*\n",
    "    colormap color (see `.Colormap.set_bad`).\n",
    "\n",
    "Returns\n",
    "-------\n",
    "`~matplotlib.collections.PathCollection`\n",
    "\n",
    "Other Parameters\n",
    "----------------\n",
    "data : indexable object, optional\n",
    "    If given, the following parameters also accept a string ``s``, which is\n",
    "    interpreted as ``data[s]`` (unless this raises an exception):\n",
    "\n",
    "    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n",
    "**kwargs : `~matplotlib.collections.Collection` properties\n",
    "\n",
    "See Also\n",
    "--------\n",
    "plot : To plot scatter plots when markers are identical in size and\n",
    "    color.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "* The `.plot` function will be faster for scatterplots where markers\n",
    "  don't vary in size or color.\n",
    "\n",
    "* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n",
    "  case all masks will be combined and only unmasked points will be\n",
    "  plotted.\n",
    "\n",
    "* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n",
    "  may be input as N-D arrays, but within scatter they will be\n",
    "  flattened. The exception is *c*, which will be flattened only if its\n",
    "  size matches the size of *x* and *y*.\n",
    "\n",
    "</code>\n",
    "<a href='#20'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f794814",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.scatter(Y_test_list, preds_list, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1d14a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>21. Library Loading</h3>  <a id='21'></a><small><a href='#top_phases'>back to top</a></small> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affbccfc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c106c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>22. Feature Engineering | Model Building and Training</h3>  <a id='22'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_selection._rfe.RFE</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Feature ranking with recursive feature elimination.\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the\n",
    "coefficients of a linear model), the goal of recursive feature elimination\n",
    "(RFE) is to select features by recursively considering smaller and smaller\n",
    "sets of features. First, the estimator is trained on the initial set of\n",
    "features and the importance of each feature is obtained either through\n",
    "any specific attribute or callable.\n",
    "Then, the least important features are pruned from current set of features.\n",
    "That procedure is recursively repeated on the pruned set until the desired\n",
    "number of features to select is eventually reached.\n",
    "\n",
    "Read more in the :ref:`User Guide <rfe>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "estimator : ``Estimator`` instance\n",
    "    A supervised learning estimator with a ``fit`` method that provides\n",
    "    information about feature importance\n",
    "    (e.g. `coef_`, `feature_importances_`).\n",
    "\n",
    "n_features_to_select : int or float, default=None\n",
    "    The number of features to select. If `None`, half of the features are\n",
    "    selected. If integer, the parameter is the absolute number of features\n",
    "    to select. If float between 0 and 1, it is the fraction of features to\n",
    "    select.\n",
    "\n",
    "    .. versionchanged:: 0.24\n",
    "       Added float values for fractions.\n",
    "\n",
    "step : int or float, default=1\n",
    "    If greater than or equal to 1, then ``step`` corresponds to the\n",
    "    (integer) number of features to remove at each iteration.\n",
    "    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n",
    "    (rounded down) of features to remove at each iteration.\n",
    "\n",
    "verbose : int, default=0\n",
    "    Controls verbosity of output.\n",
    "\n",
    "importance_getter : str or callable, default='auto'\n",
    "    If 'auto', uses the feature importance either through a `coef_`\n",
    "    or `feature_importances_` attributes of estimator.\n",
    "\n",
    "    Also accepts a string that specifies an attribute name/path\n",
    "    for extracting feature importance (implemented with `attrgetter`).\n",
    "    For example, give `regressor_.coef_` in case of\n",
    "    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n",
    "    `named_steps.clf.feature_importances_` in case of\n",
    "    class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n",
    "\n",
    "    If `callable`, overrides the default feature importance getter.\n",
    "    The callable is passed with the fitted estimator and it should\n",
    "    return importance for each feature.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "classes_ : ndarray of shape (n_classes,)\n",
    "    The classes labels. Only available when `estimator` is a classifier.\n",
    "\n",
    "estimator_ : ``Estimator`` instance\n",
    "    The fitted estimator used to select features.\n",
    "\n",
    "n_features_ : int\n",
    "    The number of selected features.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`. Only defined if the\n",
    "    underlying estimator exposes such an attribute when fit.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "ranking_ : ndarray of shape (n_features,)\n",
    "    The feature ranking, such that ``ranking_[i]`` corresponds to the\n",
    "    ranking position of the i-th feature. Selected (i.e., estimated\n",
    "    best) features are assigned rank 1.\n",
    "\n",
    "support_ : ndarray of shape (n_features,)\n",
    "    The mask of selected features.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "RFECV : Recursive feature elimination with built-in cross-validated\n",
    "    selection of the best number of features.\n",
    "SelectFromModel : Feature selection based on thresholds of importance\n",
    "    weights.\n",
    "SequentialFeatureSelector : Sequential cross-validation based feature\n",
    "    selection. Does not rely on importance weights.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Allows NaN/Inf in the input if the underlying estimator does as well.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    ".. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n",
    "       for cancer classification using support vector machines\",\n",
    "       Mach. Learn., 46(1-3), 389--422, 2002.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "The following example shows how to retrieve the 5 most informative\n",
    "features in the Friedman #1 dataset.\n",
    "\n",
    ">>> from sklearn.datasets import make_friedman1\n",
    ">>> from sklearn.feature_selection import RFE\n",
    ">>> from sklearn.svm import SVR\n",
    ">>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    ">>> estimator = SVR(kernel=\"linear\")\n",
    ">>> selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    ">>> selector = selector.fit(X, y)\n",
    ">>> selector.support_\n",
    "array([ True,  True,  True,  True,  True, False, False, False, False,\n",
    "       False])\n",
    ">>> selector.ranking_\n",
    "array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "\n",
    "</code>\n",
    "<a href='#22'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_selection._rfe.RFE.fit</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Fit the RFE model and then the underlying estimator on the selected features.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    The training input samples.\n",
    "\n",
    "y : array-like of shape (n_samples,)\n",
    "    The target values.\n",
    "\n",
    "**fit_params : dict\n",
    "    Additional parameters passed to the `fit` method of the underlying\n",
    "    estimator.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "self : object\n",
    "    Fitted estimator.\n",
    "\n",
    "</code>\n",
    "<a href='#22'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90fea8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "selector = RFE(rfr, n_features_to_select=10, step=1)\n",
    "selector.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60024c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>23. Model Building and Training</h3>  <a id='23'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_selection._rfe.RFE.predict</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Reduce X to the selected features and predict using the estimator.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : array of shape [n_samples, n_features]\n",
    "    The input samples.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "y : array of shape [n_samples]\n",
    "    The predicted target values.\n",
    "\n",
    "</code>\n",
    "<a href='#23'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55f7dd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "preds = selector.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c35f21",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>24. Model Building and Training</h3>  <a id='24'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.metrics._regression.mean_squared_error</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Mean squared error regression loss.\n",
    "\n",
    "Read more in the :ref:`User Guide <mean_squared_error>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), default=None\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
    "    Defines aggregating of multiple output values.\n",
    "    Array-like value defines weights used to average errors.\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of errors in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Errors of all outputs are averaged with uniform weight.\n",
    "\n",
    "squared : bool, default=True\n",
    "    If True returns MSE value, if False returns RMSE value.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "loss : float or ndarray of floats\n",
    "    A non-negative floating point value (the best value is 0.0), or an\n",
    "    array of floating point values, one for each individual target.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.metrics import mean_squared_error\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.375\n",
    ">>> y_true = [3, -0.5, 2, 7]\n",
    ">>> y_pred = [2.5, 0.0, 2, 8]\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.612...\n",
    ">>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
    ">>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
    ">>> mean_squared_error(y_true, y_pred)\n",
    "0.708...\n",
    ">>> mean_squared_error(y_true, y_pred, squared=False)\n",
    "0.822...\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "array([0.41666667, 1.        ])\n",
    ">>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
    "0.825...\n",
    "\n",
    "</code>\n",
    "<a href='#24'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95162829",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(preds, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62834b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
